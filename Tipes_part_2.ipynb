{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSau3yjXBNUO"
      },
      "source": [
        "#Settings Google"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2c5cYQiNV4L",
        "outputId": "766ec89b-b851-40a7-dfd4-ae192058e4ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLZnt6w30lfN",
        "outputId": "b6db0fd7-5fd1-4674-8994-bcfd180009e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_LfONqzPPXz",
        "outputId": "9519155d-a8f7-4343-d09b-fc87712f9225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/drive/MyDrive/TAabsa\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "base_dir = '/content/drive/MyDrive/TAabsa'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# Ubah direktori kerja\n",
        "os.chdir(base_dir)\n",
        "print(\"Current working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5bLS1O-55Ux"
      },
      "outputs": [],
      "source": [
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apfVSpkRMynF",
        "outputId": "adcdc5b4-2303-4a36-9072-a38f2f2cd9ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "args.json  constant.py  \u001b[0m\u001b[01;34mevaluation\u001b[0m/  \u001b[01;34mpostprocess\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/  utils.py\n",
            "\u001b[01;34mconfigs\u001b[0m/   \u001b[01;34mdata\u001b[0m/        \u001b[01;34moutput\u001b[0m/      \u001b[01;34mpreprocess\u001b[0m/   train.csv     val.csv\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT94pYOZLfPr"
      },
      "source": [
        "#Configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rbx6VXSMCVf"
      },
      "source": [
        "##na config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8TeV8p1MDdL",
        "outputId": "2f0932a6-3ce3-46ab-fdab-570c1f9bff71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data telah berhasil disimpan ke configs/na_config.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Isi data yang diinginkan\n",
        "data = [\"data/pos_tag/id/interim/data_1000.csv\"]\n",
        "\n",
        "# Konversi data ke format JSON\n",
        "json_data = json.dumps(data)\n",
        "\n",
        "# Simpan data JSON ke file\n",
        "json_file_path = \"configs/na_config.json\"\n",
        "with open(json_file_path, 'w') as json_file:\n",
        "    json_file.write(json_data)\n",
        "\n",
        "print(f\"Data telah berhasil disimpan ke {json_file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNMHjLG2NOn3"
      },
      "source": [
        "##td config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFipWZV9MLaD",
        "outputId": "c072d1b4-03f2-4bcf-e77a-10ecf49af290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data telah berhasil disimpan ke configs/td_config.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Isi data yang diinginkan\n",
        "data = [\n",
        "    {\n",
        "        \"path\": \"data/absa/id/william/train.txt\",\n",
        "        \"nt_se_order\": \"aos\",\n",
        "        \"tasks\": [\n",
        "            \"aos\",\n",
        "            \"ao\",\n",
        "            \"as\",\n",
        "            \"a\",\n",
        "            \"o\"\n",
        "        ],\n",
        "        \"n_fold\": 5,\n",
        "        \"algo\": \"round_robin\",\n",
        "        \"shuffle\": True\n",
        "    }\n",
        "]\n",
        "\n",
        "# Konversi data ke format JSON\n",
        "json_data = json.dumps(data)\n",
        "\n",
        "# Simpan data JSON ke file\n",
        "json_file_path = \"configs/td_config.json\"\n",
        "with open(json_file_path, 'w') as json_file:\n",
        "    json_file.write(json_data)\n",
        "\n",
        "print(f\"Data telah berhasil disimpan ke {json_file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nRpyOXdMEeG"
      },
      "source": [
        "##train args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtpR69T_Mr3G",
        "outputId": "77fd8a5d-8008-4507-8ae6-09fb947278f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data telah berhasil disimpan ke configs/train_args.json\n"
          ]
        }
      ],
      "source": [
        "data = {\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"learning_rate\": 3e-4,\n",
        "    \"save_total_limit\": 2,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"per_device_train_batch_size\": 8,\n",
        "    \"per_device_eval_batch_size\": 8,\n",
        "    \"save_strategy\": \"epoch\",\n",
        "    \"eval_strategy\": \"epoch\",\n",
        "    \"metric_for_best_model\": \"overall_f1_score\",\n",
        "    \"load_best_model_at_end\": True,\n",
        "    \"adam_epsilon\": 1e-08,\n",
        "    \"output_dir\": \"./output/GAS-Indo-o\",\n",
        "    \"include_inputs_for_metrics\" : True\n",
        "}\n",
        "\n",
        "# Konversi data ke format JSON\n",
        "json_data = json.dumps(data)\n",
        "\n",
        "# Simpan data JSON ke file\n",
        "json_file_path = \"configs/train_args.json\"\n",
        "with open(json_file_path, 'w') as json_file:\n",
        "    json_file.write(json_data)\n",
        "\n",
        "print(f\"Data telah berhasil disimpan ke {json_file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fmtoXofMONq"
      },
      "source": [
        "##vd config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSx1SWRvMR-P",
        "outputId": "e3e7743b-63c2-405b-e64b-7f4a0e4727d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data telah berhasil disimpan ke configs/vd_config.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Isi data yang diinginkan\n",
        "data = [\n",
        "    {\n",
        "        \"path\": \"data/absa/id/william/dev.txt\",\n",
        "        \"nt_se_order\": \"aos\",\n",
        "        \"tasks\": [\n",
        "            \"aos\",\n",
        "            \"ao\",\n",
        "            \"as\",\n",
        "            \"a\",\n",
        "            \"o\"\n",
        "        ],\n",
        "        \"n_fold\": 5,\n",
        "        \"algo\": \"round_robin\",\n",
        "        \"shuffle\": True\n",
        "    }\n",
        "]\n",
        "\n",
        "# Konversi data ke format JSON\n",
        "json_data = json.dumps(data)\n",
        "\n",
        "# Simpan data JSON ke file\n",
        "json_file_path = \"configs/vd_config.json\"\n",
        "with open(json_file_path, 'w') as json_file:\n",
        "    json_file.write(json_data)\n",
        "\n",
        "print(f\"Data telah berhasil disimpan ke {json_file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF2U7bHoMH-c"
      },
      "source": [
        "## pindahin ke drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkHSdSVwOHtL"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Misalkan direktori sumber berisi file-file yang ingin dipindahkan\n",
        "files_to_move = [\n",
        "    '/content/na_config.json',\n",
        "    '/content/td_config.json',\n",
        "    '/content/train_args.json',\n",
        "    '/content/sd_config.json'\n",
        "]\n",
        "destination_directory = '/content/drive/My Drive/TAabsa/configs'\n",
        "\n",
        "# Buat direktori tujuan jika belum ada\n",
        "os.makedirs(destination_directory, exist_ok=True)\n",
        "\n",
        "# Memindahkan setiap file ke Google Drive\n",
        "for file_path in files_to_move:\n",
        "    destination = destination_directory + file_path.split('/')[-1]\n",
        "    shutil.move(file_path, destination)\n",
        "    print(f\"{file_path} telah dipindahkan ke {destination}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vddg-yiYNTFK"
      },
      "source": [
        "#Constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1_3WsoWNWNR",
        "outputId": "89413639-e622-47b1-9c02-b3eaf1605c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting constant.py\n"
          ]
        }
      ],
      "source": [
        "# Salin kode ke dalam file utils.py\n",
        "%%writefile constant.py\n",
        "\n",
        "SENTTAG2WORD = {\"POS\": \"positive\", \"NEG\": \"negative\", \"NEU\": \"neutral\"}\n",
        "# SENTIMENT_ELEMENT = {'a' : \"aspect\", 'o' : \"opinion\", 's' : \"sentiment\", 'c' : \"category\"}\n",
        "# SENTTAG2WORD = {\"POS\": \"positif\", \"NEG\": \"negatif\", \"NEU\": \"netral\"}\n",
        "SENTIMENT_ELEMENT = {'a' : \"aspek\", 'o' : \"opini\", 's' : \"sentimen\", 'c' : \"kategori\"}\n",
        "SEP = \"####\"\n",
        "NO_TARGET = \"NULL\"\n",
        "IMPLICIT_ASPECT = \"NULL\"\n",
        "GAS_TOKEN = {\n",
        "    ',' : \"<comma>\",\n",
        "    '(' : \"<open_bracket>\",\n",
        "    ')' : \"<close_bracket>\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUVfplJGNlYi"
      },
      "source": [
        "#args json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQbN1s5XNoRC",
        "outputId": "3177dcf6-bd20-4d51-94e7-cd23678316b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data telah berhasil disimpan ke /content/drive/MyDrive/TAabsa/args.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Isi data yang diinginkan\n",
        "data = {\n",
        "    \"seed\" : 42,\n",
        "    \"n_gpu\" : \"2\",\n",
        "    \"td_config\" : \"/content/drive/MyDrive/TAabsa/configs/td_config.json\",\n",
        "    \"vd_config\" : \"/content/drive/MyDrive/TAabsa/configs/vd_config.json\",\n",
        "    \"train_args\" : \"/content/drive/MyDrive/TAabsa/configs/train_args.json\",\n",
        "    \"max_len\" : 128,\n",
        "    \"model_name_or_path\" : \"google/mt5-base\",\n",
        "    \"prompt\" : \"no_prompt\",\n",
        "    \"answer\" : \"gas\",\n",
        "    \"shuffle_train\" : True\n",
        "}\n",
        "\n",
        "# Konversi data ke format JSON\n",
        "json_data = json.dumps(data)\n",
        "\n",
        "# Simpan data JSON ke file\n",
        "json_file_path = \"/content/drive/MyDrive/TAabsa/args.json\"\n",
        "with open(json_file_path, 'w') as json_file:\n",
        "    json_file.write(json_data)\n",
        "\n",
        "print(f\"Data telah berhasil disimpan ke {json_file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6V6RNluQrI6"
      },
      "source": [
        "#Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poeOY-5-QZFo",
        "outputId": "517d35e6-5e02-4f26-d9aa-5e7ca016ca86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting evaluation/metrics.py\n"
          ]
        }
      ],
      "source": [
        "# Salin kode ke dalam file utils.py\n",
        "%%writefile evaluation/metrics.py\n",
        "from typing import List, Dict\n",
        "\n",
        "\"\"\"\n",
        "The metrics in this research follows the following rule:\n",
        "1. The definition of true positive is said that an ABSA tuple that exist in the prediction list, also exist in the target list\n",
        "2. The definition of the false positive is said that an ABSA tuple that exist in the prediction list do not exist in the target list\n",
        "3. The definition of the false negative is said that an ABSA tuple that exist in the target list do not exist in the prediction list\n",
        "\"\"\"\n",
        "\n",
        "def lower(preds_or_targets):\n",
        "    result = str(preds_or_targets)\n",
        "    result = result.lower()\n",
        "    return eval(result)\n",
        "\n",
        "def recall(predictions:List[List[Dict]],targets:List[List[Dict]]) -> float:\n",
        "    \"\"\"\n",
        "    ### DESC\n",
        "        Recall metric function for ABSA.\n",
        "    ### PARAMS\n",
        "    * predictions: List of list of prediction dictionary.\n",
        "    * targets: List of list of target dictionary.\n",
        "    ### RETURN\n",
        "    * Recall value.\n",
        "    \"\"\"\n",
        "    true_positive = 0\n",
        "    false_negative = 0\n",
        "    for prediction,target in zip(lower(predictions),lower(targets)):\n",
        "        for target_tuple in target:\n",
        "            if target_tuple in prediction:\n",
        "                true_positive += 1\n",
        "            else:\n",
        "                false_negative += 1\n",
        "    result = true_positive/(true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
        "    return result\n",
        "\n",
        "def precision(predictions:List[List[Dict]],targets:List[List[Dict]]) -> float:\n",
        "    \"\"\"\n",
        "    ### DESC\n",
        "        Precision metric function for ABSA.\n",
        "    ### PARAMS\n",
        "    * predictions: List of list of prediction dictionary.\n",
        "    * targets: List of list of target dictionary.\n",
        "    ### RETURN\n",
        "    * Precision value.\n",
        "    \"\"\"\n",
        "    true_positive = 0\n",
        "    false_positive = 0\n",
        "    for prediction,target in zip(lower(predictions),lower(targets)):\n",
        "        for prediction_tuple in prediction:\n",
        "            if prediction_tuple in target:\n",
        "                true_positive += 1\n",
        "            else:\n",
        "                false_positive += 1\n",
        "    result = true_positive/(true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
        "    return result\n",
        "\n",
        "def f1_score(predictions:List[List[Dict]],targets:List[List[Dict]]) -> float:\n",
        "    \"\"\"\n",
        "    ### DESC\n",
        "        F1 score metric function for ABSA.\n",
        "    ### PARAMS\n",
        "    * predictions: List of list of prediction dictionary.\n",
        "    * targets: List of list of target dictionary.\n",
        "    ### RETURN\n",
        "    * F1 score value.\n",
        "    \"\"\"\n",
        "    recall_value = recall(predictions,targets)\n",
        "    precision_value = precision(predictions,targets)\n",
        "    result = (2 * recall_value * precision_value)/(recall_value + precision_value) if (recall_value + precision_value) > 0 else 0\n",
        "    return result\n",
        "\n",
        "def summary_score(predictions:List[List[Dict]],targets:List[List[Dict]]) -> Dict:\n",
        "    \"\"\"\n",
        "    ### DESC\n",
        "        Score summary (recall, precision, f1 score).\n",
        "    ### PARAMS\n",
        "    * predictions: List of list of prediction dictionary.\n",
        "    * targets: List of list of target dictionary.\n",
        "    ### RETURN\n",
        "    * Score summary in a dictionary form.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"recall\" : recall(predictions,targets),\n",
        "        \"precision\" : precision(predictions,targets),\n",
        "        \"f1_score\" : f1_score(predictions,targets)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2G0Dk8iQ4VK"
      },
      "source": [
        "#Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBAMDDXIHnCj"
      },
      "outputs": [],
      "source": [
        "import evaluation\n",
        "from evaluation import metrics\n",
        "from evaluation import metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP7Jw8XTP1C2",
        "outputId": "5cd5d46e-8837-4a83-e347-d6894a9c4337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting utils.py\n"
          ]
        }
      ],
      "source": [
        "# Salin kode ke dalam file utils.py\n",
        "%%writefile utils.py\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from evaluation import metrics\n",
        "from typing import List, Dict\n",
        "\n",
        "def lower(preds_or_targets):\n",
        "        result = str(preds_or_targets)\n",
        "        result = result.lower()\n",
        "        return eval(result)\n",
        "\n",
        "def summary_score(predictions:List[List[Dict]],targets:List[List[Dict]]) -> Dict:\n",
        "        return {\n",
        "            \"recall\" : recall(predictions,targets),\n",
        "            \"precision\" : precision(predictions,targets),\n",
        "            \"f1_score\" : f1_score(predictions,targets)\n",
        "        }\n",
        "\n",
        "def recall(predictions:List[List[Dict]],targets:List[List[Dict]]) -> float:\n",
        "        true_positive = 0\n",
        "        false_negative = 0\n",
        "        for prediction,target in zip(lower(predictions),lower(targets)):\n",
        "            for target_tuple in target:\n",
        "                if target_tuple in prediction:\n",
        "                    true_positive += 1\n",
        "                else:\n",
        "                    false_negative += 1\n",
        "        result = true_positive/(true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
        "        return result\n",
        "\n",
        "def precision(predictions:List[List[Dict]],targets:List[List[Dict]]) -> float:\n",
        "        true_positive = 0\n",
        "        false_positive = 0\n",
        "        for prediction,target in zip(lower(predictions),lower(targets)):\n",
        "            for prediction_tuple in prediction:\n",
        "                if prediction_tuple in target:\n",
        "                    true_positive += 1\n",
        "                else:\n",
        "                    false_positive += 1\n",
        "        result = true_positive/(true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
        "        return result\n",
        "\n",
        "def f1_score(predictions:List[List[Dict]],targets:List[List[Dict]]) -> float:\n",
        "        recall_value = recall(predictions,targets)\n",
        "        precision_value = precision(predictions,targets)\n",
        "        result = (2 * recall_value * precision_value)/(recall_value + precision_value) if (recall_value + precision_value) > 0 else 0\n",
        "        return result\n",
        "\n",
        "def set_seed(seed: int):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def add_token_clm(model, tokenizer):\n",
        "        resize = False\n",
        "        if tokenizer.pad_token == None:\n",
        "            pad_token = \"<|pad|>\"\n",
        "            tokenizer.add_tokens([pad_token])\n",
        "            tokenizer.add_special_tokens({\"pad_token\": pad_token})\n",
        "            resize = True\n",
        "        if tokenizer.eos_token == None:\n",
        "            eos_token = \"<|endoftext|>\"\n",
        "            tokenizer.add_tokens([eos_token])\n",
        "            tokenizer.add_special_tokens({\"eos_token\": eos_token})\n",
        "            resize = True\n",
        "        if tokenizer.sep_token == None:\n",
        "            sep_token = \"<|sep|>\"\n",
        "            tokenizer.add_tokens([sep_token])\n",
        "            tokenizer.add_special_tokens({\"sep_token\": sep_token})\n",
        "            resize = True\n",
        "        if resize:\n",
        "          model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "def preprocess_logits_for_metrics(logits, targets):\n",
        "        pred_logits = logits[0] if isinstance(logits,tuple) else logits\n",
        "        pred_ids = torch.argmax(pred_logits, dim=-1)\n",
        "        return pred_ids, targets\n",
        "\n",
        "def get_task(se_order):\n",
        "        task = sorted(se_order)\n",
        "        task = ''.join(se_order)\n",
        "        return task\n",
        "\n",
        "def seperate_target_prediction_per_task(predictions, targets, se_order):\n",
        "        per_task_targets = {}\n",
        "        per_task_predictions = {}\n",
        "        for target, prediction, so in zip(targets,predictions,se_order):\n",
        "            task = get_task(so)\n",
        "            if task not in per_task_targets.keys():\n",
        "                per_task_targets[task] = []\n",
        "            if task not in per_task_predictions.keys():\n",
        "                per_task_predictions[task] = []\n",
        "            per_task_targets[task].append(target)\n",
        "            per_task_predictions[task].append(prediction)\n",
        "        return per_task_targets, per_task_predictions\n",
        "\n",
        "def preprocess_eval_preds(eval_preds, decoding_args ,tokenizer):\n",
        "        input_ids = eval_preds.inputs\n",
        "        target_ids = eval_preds.label_ids\n",
        "        pred_ids = eval_preds.predictions\n",
        "\n",
        "        # In case the model returns more than the prediction logits\n",
        "        if isinstance(input_ids, tuple):\n",
        "            input_ids = input_ids[0]\n",
        "        if isinstance(target_ids, tuple):\n",
        "            target_ids = target_ids[0]\n",
        "        if isinstance(pred_ids, tuple):\n",
        "            pred_ids = pred_ids[0]\n",
        "\n",
        "        input_ids = np.argmax(input_ids,axis=-1) if len(input_ids.shape) == 3 else input_ids # in case not predict with generate\n",
        "        target_ids = np.argmax(target_ids,axis=-1) if len(target_ids.shape) == 3 else target_ids # in case not predict with generate\n",
        "        prediction_ids = np.argmax(pred_ids,axis=-1) if len(pred_ids.shape) == 3 else pred_ids # in case not predict with generate\n",
        "\n",
        "        input_ids = [[token for token in row if token != -100] for row in input_ids]\n",
        "        target_ids = [[token for token in row if token != -100] for row in target_ids]\n",
        "        prediction_ids = [[token for token in row if token != -100] for row in prediction_ids]\n",
        "\n",
        "        inputs = tokenizer.batch_decode(input_ids,**decoding_args)\n",
        "        targets = tokenizer.batch_decode(target_ids,**decoding_args)\n",
        "        predictions = tokenizer.batch_decode(prediction_ids,**decoding_args)\n",
        "\n",
        "        return inputs, targets, predictions\n",
        "\n",
        "def compute_metrics(catch_answer, eval_preds, decoding_args, tokenizer, se_order):\n",
        "        inputs, targets, predictions = preprocess_eval_preds(eval_preds,decoding_args,tokenizer)\n",
        "\n",
        "        print(\"INPUTS >>\",inputs[0])\n",
        "        print(\"TARGETS >>\",targets[0])\n",
        "        print(\"PREDS >>\",predictions[0])\n",
        "\n",
        "        targets = [catch_answer(out,task,inputs) for out,task,inputs in zip(targets,se_order,inputs) if task != \"non_absa\"]\n",
        "        predictions = [catch_answer(out,task,inputs) for out,task,inputs in zip(predictions,se_order,inputs) if task != \"non_absa\"]\n",
        "\n",
        "        per_task_targets, per_task_predictions = seperate_target_prediction_per_task(predictions, targets, se_order)\n",
        "\n",
        "        metrics = {}\n",
        "\n",
        "        metrics[\"overall_recall\"] = recall(predictions,targets)\n",
        "        metrics[\"overall_precision\"] = precision(predictions,targets)\n",
        "        metrics[\"overall_f1_score\"] = f1_score(predictions,targets)\n",
        "\n",
        "        for task in per_task_targets.keys():\n",
        "            if task == \"non_absa\":\n",
        "                continue\n",
        "            metrics[f\"{task}_recall\"] = recall(per_task_predictions[task],per_task_targets[task])\n",
        "            metrics[f\"{task}_precision\"] = precision(per_task_predictions[task],per_task_targets[task])\n",
        "            metrics[f\"{task}_f1_score\"] = f1_score(per_task_predictions[task],per_task_targets[task])\n",
        "\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz0_TGBLQLvH"
      },
      "outputs": [],
      "source": [
        "# Membuat folder dengan nama 'preprocess'\n",
        "!mkdir preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcjQzRBIUb2r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0iuTlidM0zU"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x7su3N-VzlL"
      },
      "source": [
        "##Num targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxEpZ3rgV5Le",
        "outputId": "2c54d224-287d-491c-bf79-01f8491e0ad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting preprocess/num_targets.py\n"
          ]
        }
      ],
      "source": [
        "# Salin kode ke dalam file num_targets.py\n",
        "%%writefile preprocess/num_targets.py\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "import constant\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def process_num_targets(text:str, num_targets:List[Tuple], se_order:str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    ### DESC\n",
        "        Method for processing num targets to target in the format list of dictionaries.\n",
        "    ### PARAMS\n",
        "    * text: Text source.\n",
        "    * num_targets: Targets in the form list of tuples, may consist of aspect term or opinion term indexes.\n",
        "    * se_order: Sentiment element order, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "    ### RETURN\n",
        "    * The resultant targets in the form list of dictionaries.\n",
        "    \"\"\"\n",
        "    splitted_text = text.split()\n",
        "    result_targets = []\n",
        "    for num_target in num_targets:\n",
        "        assert len(num_target) == len(se_order) # number of element in the num targets must be the same with the task\n",
        "        target = {}\n",
        "        for i, se in enumerate(se_order): # iterate a, c, o, s\n",
        "            assert se in constant.SENTIMENT_ELEMENT\n",
        "            key = constant.SENTIMENT_ELEMENT[se]\n",
        "            if se == 'a' or se == 'o':\n",
        "                if num_target[i] != [-1]: # Implicit aspect\n",
        "                    value = ' '.join([splitted_text[j] for j in num_target[i]])\n",
        "                else:\n",
        "                    value = constant.IMPLICIT_ASPECT\n",
        "            elif se == 's':\n",
        "                value = constant.SENTTAG2WORD[num_target[i]]\n",
        "            else: # se == 'c\n",
        "                value = num_target[i]\n",
        "            target[key] = value\n",
        "        result_targets.append(target)\n",
        "    return result_targets\n",
        "\n",
        "def reduce_num_targets(num_targets:List[Tuple], src_se_order:str=\"aos\", tgt_se_order:str=\"ao\") -> List[Tuple]:\n",
        "    \"\"\"\n",
        "    ### DESC\n",
        "        Reduce num_target to fewer sentiment elements.\n",
        "    ### PARAMS\n",
        "    * num_targets: List of num_target.\n",
        "    * src_se_order: Sentiment element order from the source num_targets, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "    * tgt_se_order: Sentiment element order for the target num_targets, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "    ### RETURN\n",
        "    * Reduced num_targets.\n",
        "    \"\"\"\n",
        "    assert set(src_se_order).issubset(constant.SENTIMENT_ELEMENT)\n",
        "    assert set(tgt_se_order).issubset(src_se_order)\n",
        "    if len(num_targets) > 0:\n",
        "        assert len(src_se_order) == len(num_targets[0])\n",
        "\n",
        "    tgt_index = []\n",
        "    for se in tgt_se_order:\n",
        "        index = src_se_order.index(se)\n",
        "        tgt_index.append(index)\n",
        "\n",
        "    result = []\n",
        "    for nt in num_targets:\n",
        "        reduced_nt = []\n",
        "        for index in tgt_index:\n",
        "            reduced_nt.append(nt[index])\n",
        "        reduced_nt = tuple(reduced_nt)\n",
        "        if reduced_nt not in result:\n",
        "            result.append(reduced_nt)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQE2XzPiU0So"
      },
      "source": [
        "##ans constructor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMHBxNVwUqi7",
        "outputId": "5c3ba54b-d35e-4867-d955-42ffe850ead0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing preprocess/ans_constructor.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile preprocess/ans_constructor.py\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "import constant\n",
        "from typing import Tuple\n",
        "from num_targets import process_num_targets, reduce_num_targets\n",
        "\n",
        "class AnswerConstructor:\n",
        "    \"\"\"\n",
        "    Responsible to construct answer in to the decomposed form.\n",
        "    \"\"\"\n",
        "    def lego_absa(self, text:str, num_targets:Tuple, nt_se_order:str=\"aos\", se_order:str=\"aos\") -> str:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            LEGO-ABSA answer.\n",
        "        ### PARAMS\n",
        "        * text: Text.\n",
        "        * num_targets: Number targets. Example for aocs: [([1],[2],\"restaurant_general\",\"positive\")].\n",
        "        * nt_se_order: Sentiment order in the num_targets' tuples, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        * se_order: Sentiment element order for the decomposed answer, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        ### RETURN\n",
        "        * Decomposed answer.\n",
        "        \"\"\"\n",
        "        reduced_nt = reduce_num_targets(num_targets=num_targets,\n",
        "                                        src_se_order=nt_se_order,\n",
        "                                        tgt_se_order=se_order)\n",
        "        targets = process_num_targets(text=text,\n",
        "                                      num_targets=reduced_nt,\n",
        "                                      se_order=se_order)\n",
        "\n",
        "        if len(targets) == 0:\n",
        "            return constant.NO_TARGET\n",
        "\n",
        "        result = []\n",
        "        counter = 0\n",
        "        for t in targets:\n",
        "            constructed_t = \"\"\n",
        "            for se in se_order:\n",
        "                counter = counter % 100 # maximum index of special token in T5 is 99\n",
        "                constructed_t += ' ' + f\"<extra_id_{counter}>\" + ' ' + t[constant.SENTIMENT_ELEMENT[se]]\n",
        "                counter += 1\n",
        "            constructed_t = constructed_t.strip()\n",
        "            result.append(constructed_t)\n",
        "        result = \" ; \".join(result)\n",
        "        return result\n",
        "\n",
        "    def gas(self, text:str, num_targets:Tuple, nt_se_order:str=\"aos\", se_order:str=\"aos\") -> str:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            GAS answer.\n",
        "        ### PARAMS\n",
        "        * text: Text.\n",
        "        * num_targets: Number targets. Example for aocs: [([1],[2],\"restaurant_general\",\"positive\")].\n",
        "        * nt_se_order: Sentiment order in the num_targets' tuples, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        * se_order: Sentiment element order for the decomposed answer, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        ### RETURN\n",
        "        * Decomposed answer.\n",
        "        \"\"\"\n",
        "        reduced_nt = reduce_num_targets(num_targets=num_targets,\n",
        "                                        src_se_order=nt_se_order,\n",
        "                                        tgt_se_order=se_order)\n",
        "        targets = process_num_targets(text=text,\n",
        "                                      num_targets=reduced_nt,\n",
        "                                      se_order=se_order)\n",
        "\n",
        "        if len(targets) == 0:\n",
        "            return constant.NO_TARGET\n",
        "\n",
        "        result = []\n",
        "        for t in targets:\n",
        "            constructed_t = []\n",
        "            for se in se_order:\n",
        "                element = t[constant.SENTIMENT_ELEMENT[se]]\n",
        "\n",
        "                constructed_t.append(element)\n",
        "            constructed_t = \" , \".join(constructed_t)\n",
        "            constructed_t = f\"( {constructed_t} )\"\n",
        "            result.append(constructed_t)\n",
        "        result = \" ; \".join(result)\n",
        "        return result\n",
        "\n",
        "    def bartabsa(self, text:str, num_targets:Tuple, nt_se_order:str=\"aos\", se_order:str=\"aos\") -> str:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            BARTABSA answer.\n",
        "        ### PARAMS\n",
        "        * text: Text.\n",
        "        * num_targets: Number targets. Example for aocs: [([1],[2],\"restaurant_general\",\"positive\")].\n",
        "        * nt_se_order: Sentiment order in the num_targets' tuples, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        * se_order: Sentiment element order for the decomposed answer, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        ### RETURN\n",
        "        * Decomposed answer.\n",
        "        \"\"\"\n",
        "        reduced_nt = reduce_num_targets(num_targets=num_targets,\n",
        "                                        src_se_order=nt_se_order,\n",
        "                                        tgt_se_order=se_order)\n",
        "\n",
        "        if len(reduced_nt) == 0:\n",
        "            return \"-1\"\n",
        "\n",
        "        result = []\n",
        "\n",
        "        tgt_index = []\n",
        "        for se in se_order:\n",
        "            index = nt_se_order.index(se)\n",
        "            tgt_index.append(index)\n",
        "\n",
        "        for nt in reduced_nt:\n",
        "            for ti in tgt_index:\n",
        "                el = nt[ti]\n",
        "                if isinstance(el,list):\n",
        "                    result.append(str(el[0])) # start index\n",
        "                    result.append(str(el[-1])) # end index\n",
        "                else:\n",
        "                    result.append(el)\n",
        "        return ','.join(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycXUzHNtVdr7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHr007K4kyzy"
      },
      "source": [
        "##Read"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IVvzhRFk0-7",
        "outputId": "277fdd67-7a20-4f39-fe51-242af898ade6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting preprocess/read.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile preprocess/read.py\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "import constant\n",
        "from typing import Dict, List\n",
        "\n",
        "class DataReader:\n",
        "    \"\"\"\n",
        "    Responsible to read txt files containing ABSA dataset.\n",
        "    \"\"\"\n",
        "    def do(self, path:str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            Method to read dataset. Each line is in the format of TEXT####TARGETS .\n",
        "        ### PARAMS\n",
        "        * path: Data path.\n",
        "        ### RETURN\n",
        "        * Dataset containing \"text\" and \"num_targets\" key.\n",
        "        \"\"\"\n",
        "        assert path.endswith(\".txt\")\n",
        "        with open(path, 'r') as reader:\n",
        "            data = reader.read().strip().splitlines()\n",
        "        for i,line in enumerate(data):\n",
        "            try:\n",
        "                text, num_targets = line.split(constant.SEP)\n",
        "            except Exception as e:\n",
        "                raise ValueError(f\"Each line should be in the format 'TEXT{constant.SEP}TARGET'. Yours: {line}\")\n",
        "            num_targets = eval(num_targets)\n",
        "            data[i] = {\"text\" : text, \"num_targets\" : num_targets}\n",
        "        return data\n",
        "\n",
        "    def __call__(self, path:str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            Read dataset. Each line is in the format of TEXT####TARGETS .\n",
        "        ### PARAMS\n",
        "        * path: Data path.\n",
        "        ### RETURN\n",
        "        * Dataset containing \"text\" and \"num_targets\" key.\n",
        "        \"\"\"\n",
        "        return self.do(path=path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23CHVkuzo5nN"
      },
      "source": [
        "##Prompter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ulg7qT_Go7pe",
        "outputId": "77a331a1-b701-401c-b0f3-485a0547b0c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing preprocess/prompter.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile preprocess/prompter.py\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "import constant\n",
        "\n",
        "class Prompter:\n",
        "    \"\"\"\n",
        "    Responsible to add prompt to a text.\n",
        "    \"\"\"\n",
        "    def lego_absa(self, text:str, se_order:str=\"aos\") -> str:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            LEGO-ABSA prompt.\n",
        "        ### PARAMS\n",
        "        * text: Text.\n",
        "        * se_order: Sentiment element order, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        ### RETURN\n",
        "        * Prompted text.\n",
        "        \"\"\"\n",
        "        prompt = []\n",
        "        for counter, se in enumerate(se_order):\n",
        "            prompt.append(constant.SENTIMENT_ELEMENT[se] + \" : \" + f\"<extra_id_{counter}>\")\n",
        "        prompt = \" ,\".join(prompt)\n",
        "        result = text + \"| \" + prompt\n",
        "        return result\n",
        "\n",
        "    def gas(self, text:str, se_order:str=\"aos\") -> str:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            GAS prompt.\n",
        "        ### PARAMS\n",
        "        * text: Text.\n",
        "        * se_order: Sentiment element order, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        ### RETURN\n",
        "        * Prompted text.\n",
        "        \"\"\"\n",
        "        prompt = []\n",
        "        for se in se_order:\n",
        "            prompt.append(constant.SENTIMENT_ELEMENT[se])\n",
        "        prompt = \" , \".join(prompt)\n",
        "        prompt = f\"( {prompt} )\"\n",
        "        masked_text = text\n",
        "        result = masked_text + \"| \" + prompt\n",
        "        return result\n",
        "\n",
        "    def bartabsa(self, text:str, se_order:str=\"aos\") -> str:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            BARTABSA prompt.\n",
        "        ### PARAMS\n",
        "        * text: Text.\n",
        "        * se_order: Sentiment element order, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        ### RETURN\n",
        "        * Prompted text.\n",
        "        \"\"\"\n",
        "        prompt = []\n",
        "        for se in se_order:\n",
        "            if se == 'o' or se == 'a':\n",
        "                name = constant.SENTIMENT_ELEMENT[se]\n",
        "                start_index = name + \"_start\"\n",
        "                end_index = name + \"_end\"\n",
        "                prompt.append(start_index)\n",
        "                prompt.append(end_index)\n",
        "            else:\n",
        "                prompt.append(constant.SENTIMENT_ELEMENT[se])\n",
        "        prompt = \",\".join(prompt)\n",
        "        result = text + \"| \" + prompt\n",
        "        return result\n",
        "\n",
        "    def prefix(self, text:str, se_order:str=\"aos\") -> str:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            Prefix prompt.\n",
        "        ### PARAMS\n",
        "        * text: Text.\n",
        "        * se_order: Sentiment element order, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        ### RETURN\n",
        "        * Prompted text.\n",
        "        \"\"\"\n",
        "        prompt = []\n",
        "        for counter, se in enumerate(se_order):\n",
        "            prompt.append(constant.SENTIMENT_ELEMENT[se] + \" : \" + f\"<extra_id_{counter}>\")\n",
        "        prompt = \" ,\".join(prompt)\n",
        "        result = f\"Ekstrak ABSA dengan format >> {prompt} | \" + text\n",
        "        return result\n",
        "\n",
        "    def one_token(self, text:str, se_order:str=\"aos\") -> str:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            One token prompt.\n",
        "        ### PARAMS\n",
        "        * text: Text.\n",
        "        * se_order: Sentiment element order, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        ### RETURN\n",
        "        * Prompted text.\n",
        "        \"\"\"\n",
        "        result = f\"<{se_order}> : \" + text\n",
        "        return result\n",
        "\n",
        "    def no_prompt(self, text:str, se_order:str=\"aos\") -> str:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            No prompt.\n",
        "        ### PARAMS\n",
        "        * text: Text.\n",
        "        * se_order: Sentiment element order, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        ### RETURN\n",
        "        * Prompted text.\n",
        "        \"\"\"\n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gLrQx3KoyNG"
      },
      "source": [
        "##Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zMT-58Moq7P",
        "outputId": "a7a858d3-aa58-4d20-c667-4a4af429153f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting preprocess/augmentation.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile preprocess/augmentation.py\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict\n",
        "from .prompter import Prompter\n",
        "from .ans_constructor import AnswerConstructor\n",
        "\n",
        "available_algo = [\"random\", \"round_robin\"]\n",
        "\n",
        "class DataAugmentator:\n",
        "    \"\"\"\n",
        "    Responsible to conduct data augmentation.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.prompter = Prompter()\n",
        "        self.ans_constructor = AnswerConstructor()\n",
        "\n",
        "    def do(self, data:List[Dict], nt_se_order:str, tasks:List[Dict], n_fold:int=1, algo:str=\"round_robin\", shuffle=True) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            Method to conduct data augmentation.\n",
        "        ### PARAMS\n",
        "        * data: Data created from DataReader instance.\n",
        "        * nt_se_order: Sentiment order in the num_targets' tuples, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        * tasks: List of dictionary containing the task details. See \"task_example\" method to know the tasks example.\n",
        "        * n_fold: The amount of dataset that is multiplied.\n",
        "        * algo: Task sampling algorithm.\n",
        "        * shuffle: If shuffle is true, then the data will be shuffled using random library.\n",
        "        ### RETURN\n",
        "        * List of dictionary containing augmented dataset.\n",
        "        \"\"\"\n",
        "        result = []\n",
        "        pbar = tqdm(total=n_fold*len(data))\n",
        "\n",
        "        for i in range(len(data)):\n",
        "            for n in range(n_fold):\n",
        "                row = data[i]\n",
        "                chosen_task = None\n",
        "                if algo == \"round_robin\":\n",
        "                    chosen_task = deepcopy(tasks[((i * n_fold) + n)%len(tasks)])\n",
        "                elif algo == \"random\":\n",
        "                    chosen_task = deepcopy(random.choice(tasks))\n",
        "                else:\n",
        "                    raise NotImplementedError(f\"Available task sampling algorithm: {available_algo}\")\n",
        "\n",
        "                prompt_method = chosen_task.pop(\"prompt\")\n",
        "                answer_method = chosen_task.pop(\"answer\")\n",
        "\n",
        "                # prompter\n",
        "                prompt_func = getattr(self.prompter, prompt_method)\n",
        "                # answer\n",
        "                ans_func = getattr(self.ans_constructor, answer_method)\n",
        "\n",
        "                prompt_args = deepcopy(chosen_task)\n",
        "                prompt_args[\"text\"] = row[\"text\"]\n",
        "\n",
        "                # input\n",
        "                inputs = prompt_func(**prompt_args)\n",
        "                # output\n",
        "                out = ans_func(text=row[\"text\"],\n",
        "                            num_targets=row[\"num_targets\"],\n",
        "                            nt_se_order=nt_se_order,\n",
        "                            se_order=chosen_task[\"se_order\"])\n",
        "                result_row = {\n",
        "                    \"input\" : inputs,\n",
        "                    \"output\" : out,\n",
        "                    \"se_order\" : chosen_task[\"se_order\"]\n",
        "                }\n",
        "                if result_row not in result:\n",
        "                    result.append(result_row)\n",
        "                pbar.update(1)\n",
        "\n",
        "        if shuffle:\n",
        "            random.shuffle(result)\n",
        "        return result\n",
        "\n",
        "    def __call__(self, data:List[Dict], nt_se_order:str, tasks:List[Dict], n_fold:int, algo:str, shuffle=True) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            Conduct data augmentation.\n",
        "        ### PARAMS\n",
        "        * data: Data created from DataReader instance.\n",
        "        * nt_se_order: Sentiment order in the num_targets' tuples, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        * tasks: List of dictionary containing the task details. See \"task_example\" method to know the tasks example.\n",
        "        * n_fold: The amount of dataset that is multiplied.\n",
        "        * algo: Task sampling algorithm.\n",
        "        * shuffle: If shuffle is true, then the data will be shuffled using random library.\n",
        "        ### RETURN\n",
        "        * List of dictionary containing augmented dataset.\n",
        "        \"\"\"\n",
        "        return self.do(data=data,\n",
        "                       nt_se_order=nt_se_order,\n",
        "                       tasks=tasks,\n",
        "                       n_fold=n_fold,\n",
        "                       algo=algo,\n",
        "                       shuffle=shuffle)\n",
        "\n",
        "    def task_example(self) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Example of tasks value in the \"do\" or call method.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            {\n",
        "                \"se_order\" : \"aos\",\n",
        "                \"prompt\" : \"lego_absa\",\n",
        "                \"answer\" : \"lego_absa\"\n",
        "            },\n",
        "            {\n",
        "                \"se_order\" : \"ao\",\n",
        "                \"prompt\" : \"lego_absa\",\n",
        "                \"answer\" : \"lego_absa\"\n",
        "            },\n",
        "            {\n",
        "                \"se_order\" : \"as\",\n",
        "                \"prompt\" : \"lego_absa\",\n",
        "                \"answer\" : \"lego_absa\"\n",
        "            },\n",
        "            {\n",
        "                \"se_order\" : 'a',\n",
        "                \"prompt\" : \"lego_absa\",\n",
        "                \"answer\" : \"lego_absa\"\n",
        "            },\n",
        "            {\n",
        "                \"se_order\" : 'o',\n",
        "                \"prompt\" : \"lego_absa\",\n",
        "                \"answer\" : \"lego_absa\"\n",
        "            }\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiYLqj2jpCbp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-u-H92tpTTx"
      },
      "source": [
        "#PostProcess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChjD7bsopWBq"
      },
      "outputs": [],
      "source": [
        "mkdir postprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VrWegl2p7HO"
      },
      "source": [
        "##ans catcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXAaOIOspmaV",
        "outputId": "5e254410-cdea-4417-b7d1-0f426bf993b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing postprocess/ans_catcher.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile postprocess/ans_catcher.py\n",
        "import re\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "import constant\n",
        "from typing import List, Dict\n",
        "\n",
        "class AnswerCatcher:\n",
        "    \"\"\"\n",
        "    Responsible to catch the decomposed answer and transform it to list of dictionary.\n",
        "    \"\"\"\n",
        "    def lego_absa(self, out:str, se_order:str, text:str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            Transform LEGO-ABSA decomposed answer.\n",
        "        ### PARAMS\n",
        "        * out: Decomposed answer.\n",
        "        * se_order: Sentiment element order, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        * text: Input text.\n",
        "        ### RETURN\n",
        "        * Answer.\n",
        "        \"\"\"\n",
        "        if out == constant.NO_TARGET:\n",
        "            return []\n",
        "        pattern = r\"\"\n",
        "        for se in se_order:\n",
        "            if se != 's':\n",
        "                pattern += f\"<extra_id_\\d+>\\s*(?P<{constant.SENTIMENT_ELEMENT[se]}>[^;]+)\\s*\"\n",
        "            else:\n",
        "                # pattern += f\"<extra_id_\\d+>\\s*(?P<{constant.SENTIMENT_ELEMENT['s']}>positive|negative|neutral)\\s*\"\n",
        "                pattern += f\"<extra_id_\\d+>\\s*(?P<{constant.SENTIMENT_ELEMENT['s']}>{'|'.join(constant.SENTTAG2WORD.values())})\\s*\"\n",
        "        result = [found_iter.groupdict() for found_iter in re.finditer(pattern,out)]\n",
        "        for i in range(len(result)):\n",
        "            for k, v in result[i].items():\n",
        "                result[i][k] = v.strip()\n",
        "        return result\n",
        "\n",
        "    def gas(self, out:str, se_order:str, text:str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            Transform GAS decomposed answer.\n",
        "        ### PARAMS\n",
        "        * out: Decomposed answer.\n",
        "        * se_order: Sentiment element order, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        * text: Input text.\n",
        "        ### RETURN\n",
        "        * Answer.\n",
        "        \"\"\"\n",
        "        if out == constant.NO_TARGET:\n",
        "            return []\n",
        "        pattern = []\n",
        "        for se in se_order:\n",
        "            if se != 's':\n",
        "                pattern.append(f\"\\s*(?P<{constant.SENTIMENT_ELEMENT[se]}>[^;]+)\\s*\")\n",
        "            else:\n",
        "                # pattern.append(f\"\\s*(?P<{constant.SENTIMENT_ELEMENT['s']}>positive|negative|neutral)\\s*\")\n",
        "                pattern.append(f\"\\s*(?P<{constant.SENTIMENT_ELEMENT['s']}>{'|'.join(constant.SENTTAG2WORD.values())})\\s*\")\n",
        "        pattern = ','.join(pattern)\n",
        "        pattern = f\"\\({pattern}\\)\"\n",
        "        result = [found_iter.groupdict() for found_iter in re.finditer(pattern,out)]\n",
        "        for i in range(len(result)):\n",
        "            for k, v in result[i].items():\n",
        "                # unmask\n",
        "                for k2, v2 in constant.GAS_TOKEN.items():\n",
        "                    v = v.replace(v2,k2)\n",
        "                result[i][k] = v.strip()\n",
        "        return result\n",
        "\n",
        "    def bartabsa(self, out:str, se_order:str, text:str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            Transform BARTABSA decomposed answer.\n",
        "        ### PARAMS\n",
        "        * out: Decomposed answer.\n",
        "        * se_order: Sentiment element order, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        * text: Input text.\n",
        "        ### RETURN\n",
        "        * Answer.\n",
        "        \"\"\"\n",
        "        splitted_text = text.split()\n",
        "        if out == \"-1\":\n",
        "            return []\n",
        "        result = []\n",
        "        splitted_output = out.split(',')\n",
        "        splitted_output = [el.strip() for el in splitted_output]\n",
        "\n",
        "        chunk_size = 0\n",
        "        for se in se_order:\n",
        "            if se == 'o' or se == 'a':\n",
        "                chunk_size += 2\n",
        "            else:\n",
        "                chunk_size += 1\n",
        "\n",
        "        chunks = [\n",
        "            splitted_output[i:i+chunk_size] for i in range(0,len(splitted_output),chunk_size)\n",
        "        ]\n",
        "\n",
        "        chunks = [el for el in chunks if len(el) == chunk_size]\n",
        "\n",
        "        for el in chunks:\n",
        "            pred = {}\n",
        "            cnt_index = 0\n",
        "            is_invalid = False\n",
        "            for se in se_order:\n",
        "                if se == 'a' or se == 'o':\n",
        "                    start_index = el[cnt_index]\n",
        "                    end_index = el[cnt_index+1]\n",
        "                    cnt_index += 2\n",
        "\n",
        "                    try:\n",
        "                        start_index = int(start_index)\n",
        "                        end_index = int(end_index)\n",
        "                        if end_index < start_index:\n",
        "                            start_index, end_index = end_index, start_index\n",
        "                        if start_index == -1 or end_index == -1:\n",
        "                            pred[constant.SENTIMENT_ELEMENT[se]] = constant.IMPLICIT_ASPECT\n",
        "                        else:\n",
        "                            word = splitted_text[start_index:end_index+1]\n",
        "                            word = ' '.join(word)\n",
        "                            pred[constant.SENTIMENT_ELEMENT[se]] = word\n",
        "                    except:\n",
        "                        is_invalid = True\n",
        "                        break\n",
        "                elif se == 's':\n",
        "                    try:\n",
        "                        sentiment = constant.SENTTAG2WORD[el[cnt_index]]\n",
        "                        pred[constant.SENTIMENT_ELEMENT['s']] = sentiment\n",
        "                    except:\n",
        "                        is_invalid = True\n",
        "                        pass\n",
        "                    cnt_index += 1\n",
        "                else: # c\n",
        "                    pred[constant.SENTIMENT_ELEMENT[se]] = el[cnt_index]\n",
        "                    cnt_index += 1\n",
        "            if not is_invalid:\n",
        "                result.append(pred)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrwjMKWomUxc"
      },
      "source": [
        "##Clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJA1C3spmTJm",
        "outputId": "ff1b1e48-9b47-4804-f20b-8e3a5c1304a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting postprocess/clean.py\n"
          ]
        }
      ],
      "source": [
        "##Clean\n",
        "%%writefile postprocess/clean.py\n",
        "from typing import List\n",
        "class Cleaner:\n",
        "    \"\"\"\n",
        "    Responsible to clean the output from the generative ABSA model.\n",
        "    \"\"\"\n",
        "    def one(self, out:str, remove:List[str]=[\"</s>\",\"<pad>\"]) -> str:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            Method to clean one instance of decomposed answer.\n",
        "        ### PARAMS\n",
        "        * out: Decomposed answer.\n",
        "        * remove: Phrase/word/token that needs to be removed.\n",
        "        ### RETURN\n",
        "        * Clean answer.\n",
        "        \"\"\"\n",
        "        result = out\n",
        "        for tok in remove:\n",
        "            result = result.replace(tok, '')\n",
        "        result = result.strip()\n",
        "        return result\n",
        "\n",
        "    def many(self, outputs:List[str], remove:List[str]=[\"</s>\",\"<pad>\"]) -> List[str]:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            Method to clean many instance of decomposed answer.\n",
        "        ### PARAMS\n",
        "        * outputs: List of decomposed answer.\n",
        "        * remove: Phrase/word/token that needs to be removed.\n",
        "        ### RETURN\n",
        "        * List of clean answer.\n",
        "        \"\"\"\n",
        "        return [self.one(out=out, remove=remove) for out in outputs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VVfUBZGp9q1"
      },
      "source": [
        "## clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7ry6n50p6H5",
        "outputId": "30d6d4a0-62d0-4503-ec36-bb4ceddc6e5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing preprocess/clean.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile postprocess/clean.py\n",
        "import re\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "import constant\n",
        "from typing import List, Dict\n",
        "\n",
        "class AnswerCatcher:\n",
        "    \"\"\"\n",
        "    Responsible to catch the decomposed answer and transform it to list of dictionary.\n",
        "    \"\"\"\n",
        "    def lego_absa(self, out:str, se_order:str, text:str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            Transform LEGO-ABSA decomposed answer.\n",
        "        ### PARAMS\n",
        "        * out: Decomposed answer.\n",
        "        * se_order: Sentiment element order, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        * text: Input text.\n",
        "        ### RETURN\n",
        "        * Answer.\n",
        "        \"\"\"\n",
        "        if out == constant.NO_TARGET:\n",
        "            return []\n",
        "        pattern = r\"\"\n",
        "        for se in se_order:\n",
        "            if se != 's':\n",
        "                pattern += f\"<extra_id_\\d+>\\s*(?P<{constant.SENTIMENT_ELEMENT[se]}>[^;]+)\\s*\"\n",
        "            else:\n",
        "                # pattern += f\"<extra_id_\\d+>\\s*(?P<{constant.SENTIMENT_ELEMENT['s']}>positive|negative|neutral)\\s*\"\n",
        "                pattern += f\"<extra_id_\\d+>\\s*(?P<{constant.SENTIMENT_ELEMENT['s']}>{'|'.join(constant.SENTTAG2WORD.values())})\\s*\"\n",
        "        result = [found_iter.groupdict() for found_iter in re.finditer(pattern,out)]\n",
        "        for i in range(len(result)):\n",
        "            for k, v in result[i].items():\n",
        "                result[i][k] = v.strip()\n",
        "        return result\n",
        "\n",
        "    def gas(self, out:str, se_order:str, text:str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            Transform GAS decomposed answer.\n",
        "        ### PARAMS\n",
        "        * out: Decomposed answer.\n",
        "        * se_order: Sentiment element order, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        * text: Input text.\n",
        "        ### RETURN\n",
        "        * Answer.\n",
        "        \"\"\"\n",
        "        if out == constant.NO_TARGET:\n",
        "            return []\n",
        "        pattern = []\n",
        "        for se in se_order:\n",
        "            if se != 's':\n",
        "                pattern.append(f\"\\s*(?P<{constant.SENTIMENT_ELEMENT[se]}>[^;]+)\\s*\")\n",
        "            else:\n",
        "                # pattern.append(f\"\\s*(?P<{constant.SENTIMENT_ELEMENT['s']}>positive|negative|neutral)\\s*\")\n",
        "                pattern.append(f\"\\s*(?P<{constant.SENTIMENT_ELEMENT['s']}>{'|'.join(constant.SENTTAG2WORD.values())})\\s*\")\n",
        "        pattern = ','.join(pattern)\n",
        "        pattern = f\"\\({pattern}\\)\"\n",
        "        result = [found_iter.groupdict() for found_iter in re.finditer(pattern,out)]\n",
        "        for i in range(len(result)):\n",
        "            for k, v in result[i].items():\n",
        "                # unmask\n",
        "                for k2, v2 in constant.GAS_TOKEN.items():\n",
        "                    v = v.replace(v2,k2)\n",
        "                result[i][k] = v.strip()\n",
        "        return result\n",
        "\n",
        "    def bartabsa(self, out:str, se_order:str, text:str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            Transform BARTABSA decomposed answer.\n",
        "        ### PARAMS\n",
        "        * out: Decomposed answer.\n",
        "        * se_order: Sentiment element order, denoted by a (aspect), c (category), o (opinion), and s (sentiment).\n",
        "        * text: Input text.\n",
        "        ### RETURN\n",
        "        * Answer.\n",
        "        \"\"\"\n",
        "        splitted_text = text.split()\n",
        "        if out == \"-1\":\n",
        "            return []\n",
        "        result = []\n",
        "        splitted_output = out.split(',')\n",
        "        splitted_output = [el.strip() for el in splitted_output]\n",
        "\n",
        "        chunk_size = 0\n",
        "        for se in se_order:\n",
        "            if se == 'o' or se == 'a':\n",
        "                chunk_size += 2\n",
        "            else:\n",
        "                chunk_size += 1\n",
        "\n",
        "        chunks = [\n",
        "            splitted_output[i:i+chunk_size] for i in range(0,len(splitted_output),chunk_size)\n",
        "        ]\n",
        "\n",
        "        chunks = [el for el in chunks if len(el) == chunk_size]\n",
        "\n",
        "        for el in chunks:\n",
        "            pred = {}\n",
        "            cnt_index = 0\n",
        "            is_invalid = False\n",
        "            for se in se_order:\n",
        "                if se == 'a' or se == 'o':\n",
        "                    start_index = el[cnt_index]\n",
        "                    end_index = el[cnt_index+1]\n",
        "                    cnt_index += 2\n",
        "\n",
        "                    try:\n",
        "                        start_index = int(start_index)\n",
        "                        end_index = int(end_index)\n",
        "                        if end_index < start_index:\n",
        "                            start_index, end_index = end_index, start_index\n",
        "                        if start_index == -1 or end_index == -1:\n",
        "                            pred[constant.SENTIMENT_ELEMENT[se]] = constant.IMPLICIT_ASPECT\n",
        "                        else:\n",
        "                            word = splitted_text[start_index:end_index+1]\n",
        "                            word = ' '.join(word)\n",
        "                            pred[constant.SENTIMENT_ELEMENT[se]] = word\n",
        "                    except:\n",
        "                        is_invalid = True\n",
        "                        break\n",
        "                elif se == 's':\n",
        "                    try:\n",
        "                        sentiment = constant.SENTTAG2WORD[el[cnt_index]]\n",
        "                        pred[constant.SENTIMENT_ELEMENT['s']] = sentiment\n",
        "                    except:\n",
        "                        is_invalid = True\n",
        "                        pass\n",
        "                    cnt_index += 1\n",
        "                else: # c\n",
        "                    pred[constant.SENTIMENT_ELEMENT[se]] = el[cnt_index]\n",
        "                    cnt_index += 1\n",
        "            if not is_invalid:\n",
        "                result.append(pred)\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOAt5qx0qIHB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ii-0g7cqUFR"
      },
      "source": [
        "#Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1TIRE_Wqm_S",
        "outputId": "b1147125-8311-4918-dd21-8a9ccd02e89e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "args.json  constant.py  \u001b[0m\u001b[01;34mevaluation\u001b[0m/  \u001b[01;34mpostprocess\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/  utils.py\n",
            "\u001b[01;34mconfigs\u001b[0m/   \u001b[01;34mdata\u001b[0m/        \u001b[01;34moutput\u001b[0m/      \u001b[01;34mpreprocess\u001b[0m/   train.csv     val.csv\n"
          ]
        }
      ],
      "source": [
        "ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLlmyyCvaiOf"
      },
      "outputs": [],
      "source": [
        "# %%writefile evaluation/__init__.py\n",
        "from evaluation.metrics import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5lNLiRPj0q_"
      },
      "outputs": [],
      "source": [
        "# %%writefile postprocess/__init__.py\n",
        "from postprocess.ans_catcher import AnswerCatcher\n",
        "from postprocess.clean import Cleaner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idJXQAD6j-VH"
      },
      "outputs": [],
      "source": [
        "# %%writefile preprocess/__init__.py\n",
        "from preprocess.read import DataReader\n",
        "import preprocess.num_targets\n",
        "from preprocess.augmentation import DataAugmentator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJmnJBLxdwwm",
        "outputId": "fcf825ee-c829-4ec0-8ffe-433256281457"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9H9dqNTX-sl",
        "outputId": "415be6b9-18ab-4293-b3c0-c738f6d94c7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.31.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPrOw3k4_ISw",
        "outputId": "0ad538df-f824-45ae-94ca-ad15f314d784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9JcV3eTYz8N",
        "outputId": "1d2cef4b-7c71-44c4-8886-bf06cffdfdd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[torch] accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqk339_6HApL"
      },
      "outputs": [],
      "source": [
        "from evaluation import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651,
          "referenced_widgets": [
            "5c2feccff1484d85bf97c080b89ac547",
            "848b78cf0cac4a82a55121a58b23551e",
            "3577b1ad723b40458a1490cc261e7ac6",
            "7a2c0600f7b04e04ac3b208c8f544741",
            "9cf31f0f36b84d54bf16a9fa0e25aeb3",
            "dd2a7703ed0148c2b40f8f815a2d51d5",
            "7e3fd8b81572439f997f4736a4011236",
            "d57632adf0d246838b7de006733a187f",
            "ef9fd8174cb74a228d4c816f24c28081",
            "9da6d71e956e4a5b83688c6634b21070",
            "07685a354a794ceaab0d9bedf9c2847a",
            "a7b8aec7b3a5415b852a7179638a3e1f",
            "92021e24ebb049829570d00b7d295c77",
            "db786769c3c949c1b88ff09f3157e918",
            "283a3d3cd0074b92bbd41d8c0f64b6b1",
            "114dc92a6ce845f4b888cfdedc4ed35e",
            "be5e52b940f342cf8269becf8745043d",
            "9d004fca3368400eb2ee472cd76446a7",
            "c4a313597e5040778a0e70a3deab7a84",
            "58951bdfad414d65ac175942346b4f41",
            "2d18f5053d664d9dbca809b84d288b1f",
            "3580fdc240d34e03ae71728ef2cd0620",
            "331a9e205a994f14b35368cb85361990",
            "91714c9a67164385b02ca5c4e586c158",
            "8a2d7c67f450467cbcd99f137952f608",
            "d1b192d6ece4435f928867f8835f728d",
            "0b94d31ee8524930b0cedf8fcfeeee35",
            "6ba341d3907041ee954b82f53ce7e772",
            "c49539b59ced4031abc89a83fc3cdf52",
            "8458ae7f99e04aca8a3f629c300d7427",
            "51b907abfbac4ac6b4bb237b488d2f0c",
            "7b087a4ef4f54a44a0acab4ea3f48d13",
            "ea0fbde2cb7540eb96bbe9caac016a15",
            "12febb139f7b41208841204f4d8ebd14",
            "4df39d48dfd348dfbbd3054be0c05e7a",
            "c233e3f8ae68463db1ed1e5ae0105680",
            "94a23c0f6f9b48b3b2d3a4240f8ab7bb",
            "07594ae3ba2347edbe66aa941d101a0b",
            "d344c25b84644d21bce2b5d0126f4b10",
            "ca593e364b834ae99c9dec876b720c82",
            "70002f9be67549d8a9aa292835eb5f6f",
            "721056e6e9084dabb72bfbb0be6cdef4",
            "2f804546c280452aa6873bc1b942dab4",
            "e0c050393b0444d086c3ddee21bdf2de"
          ]
        },
        "id": "yOwSfjatqoz-",
        "outputId": "61efc7a3-a366-42cd-da8a-6a12524e780f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 15000/15000 [00:18<00:00, 815.06it/s]\n",
            "100%|| 5838/5838 [00:01<00:00, 3301.00it/s]\n",
            "100%|| 8848/8848 [00:07<00:00, 1129.54it/s]\n",
            "100%|| 5600/5600 [00:02<00:00, 2263.77it/s]\n",
            "100%|| 2935/2935 [00:00<00:00, 5909.82it/s]\n",
            "100%|| 4530/4530 [00:01<00:00, 3707.17it/s]\n",
            "100%|| 6330/6330 [00:02<00:00, 2255.76it/s]\n",
            "100%|| 3025/3025 [00:00<00:00, 5647.57it/s]\n",
            "100%|| 4285/4285 [00:01<00:00, 2591.42it/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c2feccff1484d85bf97c080b89ac547",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating CSV from Arrow format:   0%|          | 0/57 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 5000/5000 [00:02<00:00, 1735.67it/s]\n",
            "100%|| 1463/1463 [00:00<00:00, 10535.16it/s]\n",
            "100%|| 2212/2212 [00:00<00:00, 7951.97it/s] \n",
            "100%|| 50/50 [00:00<00:00, 18918.83it/s]\n",
            "100%|| 145/145 [00:00<00:00, 20641.96it/s]\n",
            "100%|| 1095/1095 [00:00<00:00, 9427.22it/s]\n",
            "100%|| 1550/1550 [00:00<00:00, 8918.40it/s] \n",
            "100%|| 740/740 [00:00<00:00, 4828.59it/s]\n",
            "100%|| 1050/1050 [00:00<00:00, 6716.08it/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7b8aec7b3a5415b852a7179638a3e1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating CSV from Arrow format:   0%|          | 0/14 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AutoModelForSeq2SeqLM Digunakan\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "331a9e205a994f14b35368cb85361990",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/56207 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12febb139f7b41208841204f4d8ebd14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/13295 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from argparse import ArgumentParser\n",
        "import os\n",
        "from datasets import Dataset\n",
        "import utils\n",
        "import preprocess\n",
        "import postprocess\n",
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from transformers import (Trainer, TrainingArguments,\n",
        "                          Seq2SeqTrainer ,Seq2SeqTrainingArguments,\n",
        "                          DataCollatorForLanguageModeling, DataCollatorForSeq2Seq,\n",
        "                          AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM)\n",
        "\n",
        "\n",
        "data_reader = preprocess.read.DataReader()\n",
        "data_augmentator = preprocess.augmentation.DataAugmentator()\n",
        "cleaner = postprocess.clean.Cleaner()\n",
        "answer_catcher = postprocess.ans_catcher.AnswerCatcher()\n",
        "\n",
        "def init_args():\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument(\"--seed\", type=int, help=\"Training seed\", default=42)\n",
        "    parser.add_argument(\"--n_gpu\", type=str, help=\"Gpu device(s) used\", default='0')\n",
        "    parser.add_argument(\"--td_config\", type=str, help=\"Path to train data configuration json file.\", default=\"configs/td_config.json\")\n",
        "    parser.add_argument(\"--vd_config\", type=str, help=\"Path to validation data configuration json file.\", default=\"configs/vd_config.json\")\n",
        "    parser.add_argument(\"--na_config\", type=str, help=\"Path to non absa data configuration json file.\", required=False)\n",
        "    parser.add_argument(\"--train_args\", type=str, help=\"Path to train configuration json file.\", default=\"configs/train_args.json\")\n",
        "    parser.add_argument(\"--max_len\", type=int, help=\"Maximum sequence length.\", default=128)\n",
        "    parser.add_argument(\"--model_name_or_path\", type=str, help=\"Model name or path.\", default=\"google/mt5-base\")\n",
        "    parser.add_argument(\"--prompt\", type=str, help=\"Prompt type [lego_absa, bartabsa, gas, prefix, one_token, no_prompt].\", default=\"lego_absa\")\n",
        "    parser.add_argument(\"--answer\", type=str, help=\"Answer type [lego_absa, bartabsa, gas].\", default=\"lego_absa\")\n",
        "    parser.add_argument(\"--remove\", type=str, nargs='+', help=\"Token/phrase/word needed to be remove, for example id_ID or en_XX in mbart output.\", default=[])\n",
        "    parser.add_argument(\"--shuffle_train\", action=\"store_true\", help=\"Shuffle overall dataset\")\n",
        "\n",
        "    # Filter out unwanted arguments passed by Jupyter\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "def set_env(args):\n",
        "    # Environment\n",
        "    utils.set_seed(args.seed)\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.n_gpu\n",
        "\n",
        "def get_data(absa_config, non_absa_config, prompt, answer, shuffle):\n",
        "    ds = []\n",
        "    for config in absa_config:\n",
        "        path = config.pop(\"path\")\n",
        "        data = data_reader.do(path)\n",
        "\n",
        "        augmentation_args = deepcopy(config)\n",
        "        for i in range(len(augmentation_args[\"tasks\"])):\n",
        "            augmentation_args[\"tasks\"][i] = {\n",
        "                \"se_order\" : augmentation_args[\"tasks\"][i],\n",
        "                \"prompt\" : prompt,\n",
        "                \"answer\" : answer\n",
        "            }\n",
        "        augmentation_args.update({\n",
        "            \"data\" : data\n",
        "        })\n",
        "\n",
        "        augmented_data = data_augmentator.do(**augmentation_args)\n",
        "        ds.extend(augmented_data)\n",
        "\n",
        "    if non_absa_config != None:\n",
        "        for path in non_absa_config:\n",
        "            df = pd.read_csv(path)\n",
        "            df[\"se_order\"] = \"non_absa\"\n",
        "            data = df.to_dict(orient=\"records\")\n",
        "            ds.extend(data)\n",
        "\n",
        "    if shuffle:\n",
        "        random.shuffle(ds)\n",
        "\n",
        "    ds = Dataset.from_list(ds)\n",
        "\n",
        "    return ds\n",
        "\n",
        "def main():\n",
        "    args = init_args()\n",
        "\n",
        "    # Set environment\n",
        "    set_env(args)\n",
        "\n",
        "    # Training data\n",
        "    ## ABSA Dataset\n",
        "    with open(args.td_config, 'r') as fp:\n",
        "        td_config = json.load(fp)\n",
        "        # print(td_config)\n",
        "    ## Non ABSA Dataset\n",
        "    na_config = None\n",
        "    if args.na_config != None:\n",
        "        with open(args.na_config, 'r') as fp:\n",
        "            na_config = json.load(fp)\n",
        "    # print(na_config)\n",
        "\n",
        "    train = get_data(td_config, na_config,\n",
        "                     prompt=args.prompt, answer=args.answer,\n",
        "                     shuffle=args.shuffle_train)\n",
        "    train.to_csv(\"train.csv\")\n",
        "    # print(train)\n",
        "    # Validation data\n",
        "    do_eval = bool(args.vd_config)\n",
        "    val = None\n",
        "    if do_eval:\n",
        "        with open(args.vd_config, 'r') as fp:\n",
        "            vd_config = json.load(fp)\n",
        "        val = get_data(vd_config, None,\n",
        "                       prompt=args.prompt, answer=args.answer,\n",
        "                       shuffle=False)\n",
        "        val.to_csv(\"val.csv\")\n",
        "\n",
        "    # Prepare tokenizer and answer utilities\n",
        "    with open(args.train_args, 'r') as fp:\n",
        "        train_args = json.load(fp)\n",
        "    output_dir = train_args[\"output_dir\"]\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
        "    encode_seq2seq = lambda x: tokenizer(x[\"input\"], text_target=x[\"output\"],\n",
        "                                    max_length=args.max_len, padding=True, truncation=True,\n",
        "                                    return_tensors=\"pt\")\n",
        "    encode_clm = lambda x: tokenizer([el[\"input\"] + ' ' + tokenizer.sep_token + ' ' + el[\"output\"] + ' ' + tokenizer.eos_token\n",
        "                                      for el in x],\n",
        "                                    max_length=args.max_len, padding=True, truncation=True,\n",
        "                                    return_tensors=\"pt\")\n",
        "\n",
        "    catch_answer_fn = getattr(answer_catcher, args.answer)\n",
        "\n",
        "    def catch_answer_seq2seq(out, se_order, text):\n",
        "        out = cleaner.one(out, remove=[tokenizer.eos_token, tokenizer.pad_token] + args.remove)\n",
        "        return catch_answer_fn(out, se_order, text)\n",
        "\n",
        "    def catch_answer_clm(out, se_order, text):\n",
        "        out = out.split(tokenizer.sep_token)[-1]\n",
        "        out = cleaner.one(out, remove=[tokenizer.eos_token, tokenizer.pad_token] + args.remove)\n",
        "        return catch_answer_fn(out, se_order, text)\n",
        "\n",
        "    decoding_args = {\n",
        "        \"skip_special_tokens\" : False\n",
        "    }\n",
        "\n",
        "    # Prepare model and training utilities\n",
        "    try:\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name_or_path)\n",
        "        print(\"AutoModelForSeq2SeqLM Digunakan\")\n",
        "        train_tok = train.map(encode_seq2seq, batched=True, remove_columns=train.column_names)\n",
        "        val_tok = None\n",
        "        data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
        "        train_args = Seq2SeqTrainingArguments(**train_args)\n",
        "        trainer_args = dict(model=model,\n",
        "                            args=train_args,\n",
        "                            tokenizer=tokenizer,\n",
        "                            data_collator=data_collator,\n",
        "                            train_dataset=train_tok,)\n",
        "        if do_eval:\n",
        "            val_tok = val.map(encode_seq2seq, batched=True, remove_columns=train.column_names)\n",
        "            trainer_args.update(dict(eval_dataset=val_tok,\n",
        "                                 compute_metrics=lambda x: utils.compute_metrics(catch_answer_seq2seq, x, decoding_args, tokenizer, val[\"se_order\"]),\n",
        "                                 preprocess_logits_for_metrics=utils.preprocess_logits_for_metrics))\n",
        "        trainer = Seq2SeqTrainer(**trainer_args)\n",
        "    except ValueError as ve:\n",
        "        try:\n",
        "            model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
        "            print(\"AutoModelForCausalLM Digunakan\")\n",
        "            utils.add_token_clm(model, tokenizer)\n",
        "            train_tok = train.map(encode_clm, batched=True, remove_columns=train.column_names)\n",
        "            val_tok = None\n",
        "            data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "            train_args = TrainingArguments(**train_args)\n",
        "            trainer_args = dict(model=model,\n",
        "                            args=train_args,\n",
        "                            tokenizer=tokenizer,\n",
        "                            data_collator=data_collator,\n",
        "                            train_dataset=train_tok,)\n",
        "            if do_eval:\n",
        "                val_tok = val.map(encode_clm, batched=True, remove_columns=train.column_names)\n",
        "                trainer_args.update(dict(eval_dataset=val_tok,\n",
        "                                 compute_metrics=lambda x: utils.compute_metrics(catch_answer_clm, x, decoding_args, tokenizer, val[\"se_order\"]),\n",
        "                                 preprocess_logits_for_metrics=utils.preprocess_logits_for_metrics))\n",
        "            trainer = Trainer(**trainer_args)\n",
        "        except ValueError as ve:\n",
        "            raise NotImplementedError(\"Only Seq2Seq and CausalLM Model\")\n",
        "\n",
        "    # Training\n",
        "    trainer.train()\n",
        "\n",
        "    # Save\n",
        "    if trainer.is_world_process_zero():\n",
        "        tokenizer.save_pretrained(save_directory=output_dir)\n",
        "        model.save_pretrained(save_directory=output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn6a9A5h7ERN",
        "outputId": "bf545353-9a20-4b16-9350-a6ff4ae67533"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.6)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.25.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycGC-Up87nis"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtPON-TG7opo"
      },
      "outputs": [],
      "source": [
        "log_dir = \"output/GAS-Indo-o/runs/Jun01_09-43-48_93788a03ba60/events.out.tfevents.1717235031.93788a03ba60.1209.0\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvpOqhgF8AoH"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=5,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[tensorboard_callback]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "e14cb473f68740099721060fc0767ade",
            "12a9a6fc78c8441980dca6248737af86",
            "c96db058f10b4916b0d2a2dedb644274",
            "d6e5294890b1479cbed9ce601c33ca47",
            "d73bd6a4d47e4663b210cb0098c367a2",
            "143cd7578cdb4bd28bed584b6940fc08"
          ]
        },
        "id": "NrHYvUdNDdkJ",
        "outputId": "e034e468-e8af-4489-aa05-88a86ed1c80a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e14cb473f68740099721060fc0767ade",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12a9a6fc78c8441980dca6248737af86",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c96db058f10b4916b0d2a2dedb644274",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6e5294890b1479cbed9ce601c33ca47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d73bd6a4d47e4663b210cb0098c367a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "143cd7578cdb4bd28bed584b6940fc08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MT5Config {\n",
            "  \"_name_or_path\": \"google/mt5-base\",\n",
            "  \"architectures\": [\n",
            "    \"MT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"mt5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250112\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
        "\n",
        "# Memuat model dan tokenizer\n",
        "model_name = 'google/mt5-base'\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Verifikasi konfigurasi model\n",
        "print(model.config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r5pApj8F1f5-",
        "outputId": "c4350a1d-fbd4-476c-bbb2-8eb51ab9bb03"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 939, in _find_spec\n",
            "AttributeError: '_OpenCVImportHook' object has no attribute 'find_spec'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-4-85331bae16f2>\", line 11, in <cell line: 11>\n",
            "    from transformers import (Trainer, TrainingArguments,\n",
            "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1525, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1535, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 41, in <module>\n",
            "    from .integrations import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1525, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1535, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\", line 72, in <module>\n",
            "    from ..trainer_callback import ProgressCallback, TrainerCallback  # noqa: E402\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\", line 28, in <module>\n",
            "    from .training_args import TrainingArguments\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\", line 73, in <module>\n",
            "    from accelerate.state import AcceleratorState, PartialState\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/__init__.py\", line 16, in <module>\n",
            "    from .accelerator import Accelerator\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 35, in <module>\n",
            "    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/checkpointing.py\", line 24, in <module>\n",
            "    from .utils import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/__init__.py\", line 193, in <module>\n",
            "    from .megatron_lm import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/megatron_lm.py\", line 33, in <module>\n",
            "    from transformers.modeling_outputs import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_outputs.py\", line 577, in <module>\n",
            "    class Seq2SeqMoEModelOutput(ModelOutput):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\", line 331, in __init_subclass__\n",
            "    _torch_pytree.register_pytree_node(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py\", line 210, in register_pytree_node\n",
            "    from . import _cxx_pytree as cxx\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_cxx_pytree.py\", line 37, in <module>\n",
            "    import optree\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1002, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 941, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap>\", line 914, in _find_spec_legacy\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1624, in getframeinfo\n",
            "    lines, lnum = findsource(frame)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 170, in findsource\n",
            "    file = getsourcefile(object) or getfile(object)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 871, in getmodule\n",
            "    if f == _filesbymodname.get(modname, None):\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_OpenCVImportHook' object has no attribute 'find_spec'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-85331bae16f2>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from transformers import (Trainer, TrainingArguments,\n\u001b[0m\u001b[1;32m     12\u001b[0m                           \u001b[0mSeq2SeqTrainer\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mSeq2SeqTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# isort: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m from .integrations import (\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mget_reporting_integration_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1524\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1534\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1535\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_callback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProgressCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainerCallback\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPREFIX_CHECKPOINT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBestRun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntervalStrategy\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIntervalStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining_args\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAcceleratorState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPartialState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributedType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAccelerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m from .big_modeling import (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcheckpointing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_accelerator_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_custom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_accelerator_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_custom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoaderDispatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepare_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_first_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/checkpointing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    192\u001b[0m )\n\u001b[0;32m--> 193\u001b[0;31m from .megatron_lm import (\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0mAbstractTrainStep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/megatron_lm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_transformers_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     from transformers.modeling_outputs import (\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mCausalLMOutputWithCrossAttentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_outputs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSeq2SeqMoEModelOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m     \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m__init_subclass__\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_torch_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2.2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 _torch_pytree.register_pytree_node(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mregister_pytree_node\u001b[0;34m(cls, flatten_fn, unflatten_fn, serialized_type_name, to_dumpable_context, from_dumpable_context, flatten_with_keys_fn)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_cxx_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcxx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_cxx_pytree.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0moptree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moptree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyTreeSpec\u001b[0m  \u001b[0;31m# direct import for type annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec_legacy\u001b[0;34m(finder, name, path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "from argparse import ArgumentParser\n",
        "import os\n",
        "from datasets import Dataset\n",
        "import utils\n",
        "import preprocess\n",
        "import postprocess\n",
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from transformers import (Trainer, TrainingArguments,\n",
        "                          Seq2SeqTrainer ,Seq2SeqTrainingArguments,\n",
        "                          DataCollatorForLanguageModeling, DataCollatorForSeq2Seq,\n",
        "                          AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM)\n",
        "\n",
        "\n",
        "data_reader = preprocess.read.DataReader()\n",
        "data_augmentator = preprocess.augmentation.DataAugmentator()\n",
        "cleaner = postprocess.clean.Cleaner()\n",
        "answer_catcher = postprocess.ans_catcher.AnswerCatcher()\n",
        "\n",
        "def init_args():\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument(\"--seed\", type=int, help=\"Training seed\", default=42)\n",
        "    parser.add_argument(\"--n_gpu\", type=str, help=\"Gpu device(s) used\", default='0')\n",
        "    parser.add_argument(\"--td_config\", type=str, help=\"Path to train data configuration json file.\", default=\"configs/td_config.json\")\n",
        "    parser.add_argument(\"--vd_config\", type=str, help=\"Path to validation data configuration json file.\", default=\"configs/vd_config.json\")\n",
        "    parser.add_argument(\"--na_config\", type=str, help=\"Path to non absa data configuration json file.\", required=False)\n",
        "    parser.add_argument(\"--train_args\", type=str, help=\"Path to train configuration json file.\", default=\"configs/train_args.json\")\n",
        "    parser.add_argument(\"--max_len\", type=int, help=\"Maximum sequence length.\", default=128)\n",
        "    parser.add_argument(\"--model_name_or_path\", type=str, help=\"Model name or path.\", default=\"google/mt5-base\")\n",
        "    parser.add_argument(\"--prompt\", type=str, help=\"Prompt type [lego_absa, bartabsa, gas, prefix, one_token, no_prompt].\", default=\"lego_absa\")\n",
        "    parser.add_argument(\"--answer\", type=str, help=\"Answer type [lego_absa, bartabsa, gas].\", default=\"lego_absa\")\n",
        "    parser.add_argument(\"--remove\", type=str, nargs='+', help=\"Token/phrase/word needed to be remove, for example id_ID or en_XX in mbart output.\", default=[])\n",
        "    parser.add_argument(\"--shuffle_train\", action=\"store_true\", help=\"Shuffle overall dataset\")\n",
        "\n",
        "    # Filter out unwanted arguments passed by Jupyter\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "def set_env(args):\n",
        "    # Environment\n",
        "    utils.set_seed(args.seed)\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.n_gpu\n",
        "\n",
        "def get_data(absa_config, non_absa_config, prompt, answer, shuffle):\n",
        "    ds = []\n",
        "    for config in absa_config:\n",
        "        path = config.pop(\"path\")\n",
        "        data = data_reader.do(path)\n",
        "\n",
        "        augmentation_args = deepcopy(config)\n",
        "        for i in range(len(augmentation_args[\"tasks\"])):\n",
        "            augmentation_args[\"tasks\"][i] = {\n",
        "                \"se_order\" : augmentation_args[\"tasks\"][i],\n",
        "                \"prompt\" : prompt,\n",
        "                \"answer\" : answer\n",
        "            }\n",
        "        augmentation_args.update({\n",
        "            \"data\" : data\n",
        "        })\n",
        "\n",
        "        augmented_data = data_augmentator.do(**augmentation_args)\n",
        "        ds.extend(augmented_data)\n",
        "\n",
        "    if non_absa_config != None:\n",
        "        for path in non_absa_config:\n",
        "            df = pd.read_csv(path)\n",
        "            df[\"se_order\"] = \"non_absa\"\n",
        "            data = df.to_dict(orient=\"records\")\n",
        "            ds.extend(data)\n",
        "\n",
        "    if shuffle:\n",
        "        random.shuffle(ds)\n",
        "\n",
        "    ds = Dataset.from_list(ds)\n",
        "\n",
        "    return ds\n",
        "\n",
        "def main():\n",
        "    args = init_args()\n",
        "\n",
        "    # Set environment\n",
        "    set_env(args)\n",
        "\n",
        "    # Training data\n",
        "    ## ABSA Dataset\n",
        "    with open(args.td_config, 'r') as fp:\n",
        "        td_config = json.load(fp)\n",
        "        # print(td_config)\n",
        "    ## Non ABSA Dataset\n",
        "    na_config = None\n",
        "    if args.na_config != None:\n",
        "        with open(args.na_config, 'r') as fp:\n",
        "            na_config = json.load(fp)\n",
        "            # print(na_config)\n",
        "\n",
        "    train = get_data(td_config, na_config,\n",
        "                     prompt=args.prompt, answer=args.answer,\n",
        "                     shuffle=args.shuffle_train)\n",
        "    train.to_csv(\"train.csv\")\n",
        "    # print(train)\n",
        "    # Validation data\n",
        "    do_eval = bool(args.vd_config)\n",
        "    val = None\n",
        "    if do_eval:\n",
        "        with open(args.vd_config, 'r') as fp:\n",
        "            vd_config = json.load(fp)\n",
        "        val = get_data(vd_config, None,\n",
        "                       prompt=args.prompt, answer=args.answer,\n",
        "                       shuffle=False)\n",
        "        val.to_csv(\"val.csv\")\n",
        "\n",
        "    # Prepare tokenizer and answer utilities\n",
        "    with open(args.train_args, 'r') as fp:\n",
        "        train_args = json.load(fp)\n",
        "    output_dir = train_args[\"output_dir\"]\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
        "    encode_seq2seq = lambda x: tokenizer(x[\"input\"], text_target=x[\"output\"],\n",
        "                                    max_length=args.max_len, padding=True, truncation=True,\n",
        "                                    return_tensors=\"pt\")\n",
        "    encode_clm = lambda x: tokenizer([el[\"input\"] + ' ' + tokenizer.sep_token + ' ' + el[\"output\"] + ' ' + tokenizer.eos_token\n",
        "                                      for el in x],\n",
        "                                    max_length=args.max_len, padding=True, truncation=True,\n",
        "                                    return_tensors=\"pt\")\n",
        "\n",
        "    catch_answer_fn = getattr(answer_catcher, args.answer)\n",
        "\n",
        "    def catch_answer_seq2seq(out, se_order, text):\n",
        "        out = cleaner.one(out, remove=[tokenizer.eos_token, tokenizer.pad_token] + args.remove)\n",
        "        return catch_answer_fn(out, se_order, text)\n",
        "\n",
        "    def catch_answer_clm(out, se_order, text):\n",
        "        out = out.split(tokenizer.sep_token)[-1]\n",
        "        out = cleaner.one(out, remove=[tokenizer.eos_token, tokenizer.pad_token] + args.remove)\n",
        "        return catch_answer_fn(out, se_order, text)\n",
        "\n",
        "    decoding_args = {\n",
        "        \"skip_special_tokens\" : False\n",
        "    }\n",
        "\n",
        "    # Prepare model and training utilities\n",
        "    try:\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name_or_path)\n",
        "        train_tok = train.map(encode_seq2seq, batched=True, remove_columns=train.column_names)\n",
        "        val_tok = None\n",
        "        data_collator = DataCollatorForSeq2Seq(tokenizer)\n",
        "        train_args = Seq2SeqTrainingArguments(**train_args)\n",
        "        trainer_args = dict(model=model,\n",
        "                            args=train_args,\n",
        "                            tokenizer=tokenizer,\n",
        "                            data_collator=data_collator,\n",
        "                            train_dataset=train_tok,)\n",
        "        if do_eval:\n",
        "            val_tok = val.map(encode_seq2seq, batched=True, remove_columns=train.column_names)\n",
        "            trainer_args.update(dict(eval_dataset=val_tok,\n",
        "                                 compute_metrics=lambda x: utils.compute_metrics(catch_answer_seq2seq, x, decoding_args, tokenizer, val[\"se_order\"]),\n",
        "                                 preprocess_logits_for_metrics=utils.preprocess_logits_for_metrics))\n",
        "        trainer = Seq2SeqTrainer(**trainer_args)\n",
        "    except ValueError as ve:\n",
        "        try:\n",
        "            model = AutoModelForCausalLM.from_pretrained(args.model_name_or_path)\n",
        "            utils.add_token_clm(model, tokenizer)\n",
        "            train_tok = train.map(encode_clm, batched=True, remove_columns=train.column_names)\n",
        "            val_tok = None\n",
        "            data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "            train_args = TrainingArguments(**train_args)\n",
        "            trainer_args = dict(model=model,\n",
        "                            args=train_args,\n",
        "                            tokenizer=tokenizer,\n",
        "                            data_collator=data_collator,\n",
        "                            train_dataset=train_tok,)\n",
        "            if do_eval:\n",
        "                val_tok = val.map(encode_clm, batched=True, remove_columns=train.column_names)\n",
        "                trainer_args.update(dict(eval_dataset=val_tok,\n",
        "                                 compute_metrics=lambda x: utils.compute_metrics(catch_answer_clm, x, decoding_args, tokenizer, val[\"se_order\"]),\n",
        "                                 preprocess_logits_for_metrics=utils.preprocess_logits_for_metrics))\n",
        "            trainer = Trainer(**trainer_args)\n",
        "        except ValueError as ve:\n",
        "            raise NotImplementedError(\"Only Seq2Seq and CausalLM Model\")\n",
        "\n",
        "    # Training\n",
        "    trainer.train()\n",
        "\n",
        "    # Save\n",
        "    if trainer.is_world_process_zero():\n",
        "        tokenizer.save_pretrained(save_directory=output_dir)\n",
        "        model.save_pretrained(save_directory=output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "rSeVf3Os_Zzo",
        "outputId": "faec08b4-c16d-45f4-d1d6-4b357fb8b1e1"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "_BaseAutoModelClass.from_pretrained() missing 1 required positional argument: 'pretrained_model_name_or_path'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-432f7fe8bffb>\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-432f7fe8bffb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m     }\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: _BaseAutoModelClass.from_pretrained() missing 1 required positional argument: 'pretrained_model_name_or_path'"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from argparse import ArgumentParser\n",
        "import os\n",
        "from datasets import Dataset\n",
        "import utils\n",
        "import preprocess\n",
        "import postprocess\n",
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "from copy import deepcopy\n",
        "from transformers import (Trainer, TrainingArguments,\n",
        "                          Seq2SeqTrainer ,Seq2SeqTrainingArguments,\n",
        "                          DataCollatorForLanguageModeling, DataCollatorForSeq2Seq,\n",
        "                          AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM)\n",
        "\n",
        "\n",
        "data_reader = preprocess.read.DataReader()\n",
        "data_augmentator = preprocess.augmentation.DataAugmentator()\n",
        "cleaner = postprocess.clean.Cleaner()\n",
        "answer_catcher = postprocess.ans_catcher.AnswerCatcher()\n",
        "def init_args():\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument(\"--seed\", type=int, help=\"Training seed\", default=42)\n",
        "    parser.add_argument(\"--n_gpu\", type=str, help=\"Gpu device(s) used\", default='0')\n",
        "    parser.add_argument(\"--td_config\", type=str, help=\"Path to train data configuration json file.\", default=\"configs/td_config.json\")\n",
        "    parser.add_argument(\"--vd_config\", type=str, help=\"Path to validation data configuration json file.\", default=\"configs/vd_config.json\")\n",
        "    parser.add_argument(\"--na_config\", type=str, help=\"Path to non absa data configuration json file.\", required=False)\n",
        "    parser.add_argument(\"--train_args\", type=str, help=\"Path to train configuration json file.\", default=\"configs/train_args.json\")\n",
        "    parser.add_argument(\"--max_len\", type=int, help=\"Maximum sequence length.\", default=128)\n",
        "    parser.add_argument(\"--model_name_or_path\", type=str, help=\"Model name or path.\", default=\"google/mt5-base\")\n",
        "    parser.add_argument(\"--prompt\", type=str, help=\"Prompt type [lego_absa, bartabsa, gas, prefix, one_token, no_prompt].\", default=\"lego_absa\")\n",
        "    parser.add_argument(\"--answer\", type=str, help=\"Answer type [lego_absa, bartabsa, gas].\", default=\"lego_absa\")\n",
        "    parser.add_argument(\"--remove\", type=str, nargs='+', help=\"Token/phrase/word needed to be remove, for example id_ID or en_XX in mbart output.\", default=[])\n",
        "    parser.add_argument(\"--shuffle_train\", action=\"store_true\", help=\"Shuffle overall dataset\")\n",
        "\n",
        "    # Filter out unwanted arguments passed by Jupyter\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "def set_env(args):\n",
        "    # Environment\n",
        "    utils.set_seed(args.seed)\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.n_gpu\n",
        "\n",
        "def main():\n",
        "    args = init_args()\n",
        "\n",
        "    # Set environment\n",
        "    set_env(args)\n",
        "    with open(args.train_args, 'r') as fp:\n",
        "          train_args = json.load(fp)\n",
        "    output_dir = train_args[\"output_dir\"]\n",
        "    def catch_answer_seq2seq(out, se_order, text):\n",
        "        out = cleaner.one(out, remove=[tokenizer.eos_token, tokenizer.pad_token] + args.remove)\n",
        "        return catch_answer_fn(out, se_order, text)\n",
        "\n",
        "    def catch_answer_clm(out, se_order, text):\n",
        "        out = out.split(tokenizer.sep_token)[-1]\n",
        "        out = cleaner.one(out, remove=[tokenizer.eos_token, tokenizer.pad_token] + args.remove)\n",
        "        return catch_answer_fn(out, se_order, text)\n",
        "\n",
        "    decoding_args = {\n",
        "        \"skip_special_tokens\" : False\n",
        "    }\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(save_directory=output_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(save_directory=output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8SsRZvBTDui"
      },
      "source": [
        "#trailn error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg9aEu9w5MpI",
        "outputId": "a0086ee8-5996-406b-b643-0889463de669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "args.json  constant.py  \u001b[0m\u001b[01;34mevaluation\u001b[0m/  \u001b[01;34mpostprocess\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/  utils.py\n",
            "\u001b[01;34mconfigs\u001b[0m/   \u001b[01;34mdata\u001b[0m/        \u001b[01;34moutput\u001b[0m/      \u001b[01;34mpreprocess\u001b[0m/   train.csv     val.csv\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "qXEz4wnGgzEv",
        "outputId": "43b0951c-1e53-4cd7-a90a-e59234e32934"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'evaluation.metrics' has no attribute 'peqi'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-78df724c9f89>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# from metrics import *\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeqi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'evaluation.metrics' has no attribute 'peqi'"
          ]
        }
      ],
      "source": [
        "import evaluation\n",
        "from evaluation import metrics\n",
        "\n",
        "# from metrics import *\n",
        "metrics.peqi()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "z5pMzVo2Rwf8",
        "outputId": "8bce6e70-31e7-45f3-9fe4-b12b47dfdca1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'init_args' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3bde96b81e37>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Set environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mset_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'init_args' is not defined"
          ]
        }
      ],
      "source": [
        "args = init_args()\n",
        "\n",
        "# Set environment\n",
        "set_env(args)\n",
        "\n",
        "# Training data\n",
        "## ABSA Dataset\n",
        "with open(args.td_config, 'r') as fp:\n",
        "    td_config = json.load(fp)\n",
        "    # print(td_config)\n",
        "## Non ABSA Dataset\n",
        "na_config = None\n",
        "if args.na_config != None:\n",
        "    with open(args.na_config, 'r') as fp:\n",
        "        na_config = json.load(fp)\n",
        "        # print(na_config)\n",
        "\n",
        "train = get_data(td_config, na_config,\n",
        "                  prompt=args.prompt, answer=args.answer,\n",
        "                  shuffle=args.shuffle_train)\n",
        "train.to_csv(\"train.csv\")\n",
        "# print(train)\n",
        "# Validation data\n",
        "do_eval = bool(args.vd_config)\n",
        "val = None\n",
        "if do_eval:\n",
        "    with open(args.vd_config, 'r') as fp:\n",
        "        vd_config = json.load(fp)\n",
        "    val = get_data(vd_config, None,\n",
        "                    prompt=args.prompt, answer=args.answer,\n",
        "                    shuffle=False)\n",
        "    val.to_csv(\"val.csv\")\n",
        "\n",
        "# Prepare tokenizer and answer utilities\n",
        "with open(args.train_args, 'r') as fp:\n",
        "    train_args = json.load(fp)\n",
        "output_dir = train_args[\"output_dir\"]\n",
        "# print(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "TDQU8IKWTG8G",
        "outputId": "2ce72493-6f1d-4fcd-b350-f656d30c38da"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'output_dir'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-5e9220bf1a35>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_dir\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'output_dir'"
          ]
        }
      ],
      "source": [
        "train_args[0][\"output_dir\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QLVnEVjTliM",
        "outputId": "8cf35b2f-7e39-4a40-d7c1-c41047092289"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Konten train_args: [{'path': 'data/absa/id/william/train.txt', 'nt_se_order': 'aos', 'tasks': ['aos', 'ao', 'as', 'a', 'o'], 'n_fold': 5, 'algo': 'round_robin', 'shuffle': True}, {'path': 'data/absa/en/zhang/interim/interim_2/rest15/train.txt', 'nt_se_order': 'acso', 'tasks': ['acos', 'ao', 'as', 'cs', 'a', 'o', 'c'], 'n_fold': 7, 'algo': 'round_robin', 'shuffle': True}, {'path': 'data/absa/en/zhang/interim/interim_2/rest16/train.txt', 'nt_se_order': 'acso', 'tasks': ['acos', 'ao', 'as', 'cs', 'a', 'o', 'c'], 'n_fold': 7, 'algo': 'round_robin', 'shuffle': True}, {'path': 'data/absa/en/wan/interim/rest15/train.txt', 'nt_se_order': 'acs', 'tasks': ['acs', 'as', 'cs', 'a', 'c'], 'n_fold': 5, 'algo': 'round_robin', 'shuffle': True}, {'path': 'data/absa/en/wan/interim/rest16/train.txt', 'nt_se_order': 'acs', 'tasks': ['acs', 'as', 'cs', 'a', 'c'], 'n_fold': 5, 'algo': 'round_robin', 'shuffle': True}, {'path': 'data/absa/en/peng/14lap/train_triplets.txt', 'nt_se_order': 'aos', 'tasks': ['aos', 'ao', 'as', 'a', 'o'], 'n_fold': 5, 'algo': 'round_robin', 'shuffle': True}, {'path': 'data/absa/en/peng/14res/train_triplets.txt', 'nt_se_order': 'aos', 'tasks': ['aos', 'ao', 'as', 'a', 'o'], 'n_fold': 5, 'algo': 'round_robin', 'shuffle': True}, {'path': 'data/absa/en/peng/15res/train_triplets.txt', 'nt_se_order': 'aos', 'tasks': ['aos', 'ao', 'as', 'a', 'o'], 'n_fold': 5, 'algo': 'round_robin', 'shuffle': True}, {'path': 'data/absa/en/peng/16res/train_triplets.txt', 'nt_se_order': 'aos', 'tasks': ['aos', 'ao', 'as', 'a', 'o'], 'n_fold': 5, 'algo': 'round_robin', 'shuffle': True}]\n",
            "Output directory: data/absa/id/william/train.txt\n",
            "Output directory: data/absa/en/zhang/interim/interim_2/rest15/train.txt\n",
            "Output directory: data/absa/en/zhang/interim/interim_2/rest16/train.txt\n",
            "Output directory: data/absa/en/wan/interim/rest15/train.txt\n",
            "Output directory: data/absa/en/wan/interim/rest16/train.txt\n",
            "Output directory: data/absa/en/peng/14lap/train_triplets.txt\n",
            "Output directory: data/absa/en/peng/14res/train_triplets.txt\n",
            "Output directory: data/absa/en/peng/15res/train_triplets.txt\n",
            "Output directory: data/absa/en/peng/16res/train_triplets.txt\n"
          ]
        }
      ],
      "source": [
        "with open(args.train_args, 'r') as fp:\n",
        "        train_args = json.load(fp)\n",
        "\n",
        "        # Debugging: cetak isi train_args\n",
        "        print(\"Konten train_args:\", train_args)\n",
        "\n",
        "        # Pastikan train_args adalah list\n",
        "        if not isinstance(train_args, list):\n",
        "            raise TypeError(\"Expected train_args to be a list but got a different type.\")\n",
        "\n",
        "        # Loop melalui setiap elemen dalam list\n",
        "        for train_arg in train_args:\n",
        "            # Pastikan setiap elemen adalah dictionary\n",
        "            if not isinstance(train_arg, dict):\n",
        "                raise TypeError(\"Expected elements of train_args to be dictionaries.\")\n",
        "\n",
        "            output_dir = train_arg[\"path\"]  # Misalnya kita ingin mengakses 'path' sebagai 'output_dir'\n",
        "            print(\"Output directory:\", output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufXYQJ2tiv-G"
      },
      "outputs": [],
      "source": [
        "data_reader = preprocess.read.DataReader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btfUTmxtl-Or"
      },
      "outputs": [],
      "source": [
        "import postprocess.clean\n",
        "import postprocess.ans_catcher\n",
        "ans_catcher = postprocess.ans_catcher\n",
        "cleans = postprocess.clean\n",
        "from typing import List\n",
        "\n",
        "class Cleaner:\n",
        "    \"\"\"\n",
        "    Responsible to clean the output from the generative ABSA model.\n",
        "    \"\"\"\n",
        "    def one(self, out:str, remove:List[str]=[\"</s>\",\"<pad>\"]) -> str:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            Method to clean one instance of decomposed answer.\n",
        "        ### PARAMS\n",
        "        * out: Decomposed answer.\n",
        "        * remove: Phrase/word/token that needs to be removed.\n",
        "        ### RETURN\n",
        "        * Clean answer.\n",
        "        \"\"\"\n",
        "        result = out\n",
        "        for tok in remove:\n",
        "            result = result.replace(tok, '')\n",
        "        result = result.strip()\n",
        "        return result\n",
        "\n",
        "    def many(self, outputs:List[str], remove:List[str]=[\"</s>\",\"<pad>\"]) -> List[str]:\n",
        "        \"\"\"\n",
        "        ### DESC\n",
        "            Method to clean many instance of decomposed answer.\n",
        "        ### PARAMS\n",
        "        * outputs: List of decomposed answer.\n",
        "        * remove: Phrase/word/token that needs to be removed.\n",
        "        ### RETURN\n",
        "        * List of clean answer.\n",
        "        \"\"\"\n",
        "        return [self.one(out=out, remove=remove) for out in outputs]\n",
        "cleaner = Cleaner()\n",
        "answer_catcher = ans_catcher.AnswerCatcher()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsuD7eUufGlY",
        "outputId": "795eddbd-99ba-4c45-ebe3-f13519f137ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "args.json  constant.py  \u001b[0m\u001b[01;34mevaluation\u001b[0m/   \u001b[01;34mpreprocess\u001b[0m/   utils.py\n",
            "\u001b[01;34mconfigs\u001b[0m/   \u001b[01;34mdata\u001b[0m/        \u001b[01;34mpostprocess\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvCd4vaLeNUZ",
        "outputId": "118ae395-e4bc-461d-8231-217a657c630c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'path': 'data/absa/id/william/train.txt', 'nt_se_order': 'aos', 'tasks': ['aos', 'ao', 'as', 'a', 'o'], 'n_fold': 5, 'algo': 'round_robin', 'shuffle': True}, {'path': 'data/absa/en/zhang/interim/interim_2/rest15/train.txt', 'nt_se_order': 'acso', 'tasks': ['acos', 'ao', 'as', 'cs', 'a', 'o', 'c'], 'n_fold': 7, 'algo': 'round_robin', 'shuffle': True}, {'path': 'data/absa/en/zhang/interim/interim_2/rest16/train.txt', 'nt_se_order': 'acso', 'tasks': ['acos', 'ao', 'as', 'cs', 'a', 'o', 'c'], 'n_fold': 7, 'algo': 'round_robin', 'shuffle': True}, {'path': 'data/absa/en/wan/interim/rest15/train.txt', 'nt_se_order': 'acs', 'tasks': ['acs', 'as', 'cs', 'a', 'c'], 'n_fold': 5, 'algo': 'round_robin', 'shuffle': True}, {'path': 'data/absa/en/wan/interim/rest16/train.txt', 'nt_se_order': 'acs', 'tasks': ['acs', 'as', 'cs', 'a', 'c'], 'n_fold': 5, 'algo': 'round_robin', 'shuffle': True}, {'path': 'data/absa/en/peng/14lap/train_triplets.txt', 'nt_se_order': 'aos', 'tasks': ['aos', 'ao', 'as', 'a', 'o'], 'n_fold': 5, 'algo': 'round_robin', 'shuffle': True}, {'path': 'data/absa/en/peng/14res/train_triplets.txt', 'nt_se_order': 'aos', 'tasks': ['aos', 'ao', 'as', 'a', 'o'], 'n_fold': 5, 'algo': 'round_robin', 'shuffle': True}, {'path': 'data/absa/en/peng/15res/train_triplets.txt', 'nt_se_order': 'aos', 'tasks': ['aos', 'ao', 'as', 'a', 'o'], 'n_fold': 5, 'algo': 'round_robin', 'shuffle': True}, {'path': 'data/absa/en/peng/16res/train_triplets.txt', 'nt_se_order': 'aos', 'tasks': ['aos', 'ao', 'as', 'a', 'o'], 'n_fold': 5, 'algo': 'round_robin', 'shuffle': True}]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import sys\n",
        "from argparse import ArgumentParser, Namespace\n",
        "\n",
        "def init_args() -> Namespace:\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument(\"--seed\", type=int, help=\"Training seed\", default=42)\n",
        "    parser.add_argument(\"--n_gpu\", type=str, help=\"Gpu device(s) used\", default='0')\n",
        "    parser.add_argument(\"--td_config\", type=str, help=\"Path to train data configuration json file.\", default=\"configs/td_config.json\")\n",
        "    parser.add_argument(\"--vd_config\", type=str, help=\"Path to validation data configuration json file.\", required=False)\n",
        "    parser.add_argument(\"--na_config\", type=str, help=\"Path to non absa data configuration json file.\", required=False)\n",
        "    parser.add_argument(\"--train_args\", type=str, help=\"Path to train configuration json file.\", default=\"configs/train_args.json\")\n",
        "    parser.add_argument(\"--max_len\", type=int, help=\"Maximum sequence length.\", default=128)\n",
        "    parser.add_argument(\"--model_name_or_path\", type=str, help=\"Model name or path.\", default=\"google/mt5-base\")\n",
        "    parser.add_argument(\"--prompt\", type=str, help=\"Prompt type [lego_absa, bartabsa, gas, prefix, one_token, no_prompt].\", default=\"lego_absa\")\n",
        "    parser.add_argument(\"--answer\", type=str, help=\"Answer type [lego_absa, bartabsa, gas].\", default=\"lego_absa\")\n",
        "    parser.add_argument(\"--remove\", type=str, nargs='+', help=\"Token/phrase/word needed to be remove, for example id_ID or en_XX in mbart output.\", default=[])\n",
        "    parser.add_argument(\"--shuffle_train\", action=\"store_true\", help=\"Shuffle overall dataset\")\n",
        "\n",
        "    # Filter out unwanted arguments passed by Jupyter\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "args = init_args()\n",
        "\n",
        "# Open and read the JSON configuration file\n",
        "with open(args.td_config, 'r') as fp:\n",
        "    td_config = json.load(fp)\n",
        "    print(td_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYXlyU8LkutD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbrqtI9LDA3q",
        "outputId": "834ef642-bc1d-481c-ccd6-0ba686895ebc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: transformers\n",
            "Version: 4.41.2\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "pip show transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQUqDFN-DExh",
        "outputId": "c20f0660-d3df-4886-fc61-21a59342f1f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MT5Config {\n",
            "  \"_name_or_path\": \"/home/patrick/hugging_face/t5/mt5-base\",\n",
            "  \"architectures\": [\n",
            "    \"MT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"mt5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250112\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import MT5Config\n",
        "\n",
        "config = MT5Config.from_pretrained('google/mt5-base')\n",
        "print(config)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5Rbx6VXSMCVf",
        "1nRpyOXdMEeG",
        "1fmtoXofMONq",
        "xF2U7bHoMH-c",
        "Vddg-yiYNTFK",
        "uUVfplJGNlYi",
        "i6V6RNluQrI6",
        "A2G0Dk8iQ4VK",
        "W0iuTlidM0zU",
        "NQE2XzPiU0So",
        "GHr007K4kyzy",
        "23CHVkuzo5nN",
        "0gLrQx3KoyNG",
        "6-u-H92tpTTx",
        "6VrWegl2p7HO"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07594ae3ba2347edbe66aa941d101a0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07685a354a794ceaab0d9bedf9c2847a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b94d31ee8524930b0cedf8fcfeeee35": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "114dc92a6ce845f4b888cfdedc4ed35e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12febb139f7b41208841204f4d8ebd14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4df39d48dfd348dfbbd3054be0c05e7a",
              "IPY_MODEL_c233e3f8ae68463db1ed1e5ae0105680",
              "IPY_MODEL_94a23c0f6f9b48b3b2d3a4240f8ab7bb"
            ],
            "layout": "IPY_MODEL_07594ae3ba2347edbe66aa941d101a0b"
          }
        },
        "283a3d3cd0074b92bbd41d8c0f64b6b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d18f5053d664d9dbca809b84d288b1f",
            "placeholder": "",
            "style": "IPY_MODEL_3580fdc240d34e03ae71728ef2cd0620",
            "value": "14/14[00:00&lt;00:00,24.40ba/s]"
          }
        },
        "2d18f5053d664d9dbca809b84d288b1f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f804546c280452aa6873bc1b942dab4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "331a9e205a994f14b35368cb85361990": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91714c9a67164385b02ca5c4e586c158",
              "IPY_MODEL_8a2d7c67f450467cbcd99f137952f608",
              "IPY_MODEL_d1b192d6ece4435f928867f8835f728d"
            ],
            "layout": "IPY_MODEL_0b94d31ee8524930b0cedf8fcfeeee35"
          }
        },
        "3577b1ad723b40458a1490cc261e7ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d57632adf0d246838b7de006733a187f",
            "max": 57,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef9fd8174cb74a228d4c816f24c28081",
            "value": 57
          }
        },
        "3580fdc240d34e03ae71728ef2cd0620": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4df39d48dfd348dfbbd3054be0c05e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d344c25b84644d21bce2b5d0126f4b10",
            "placeholder": "",
            "style": "IPY_MODEL_ca593e364b834ae99c9dec876b720c82",
            "value": "Map:100%"
          }
        },
        "51b907abfbac4ac6b4bb237b488d2f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58951bdfad414d65ac175942346b4f41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c2feccff1484d85bf97c080b89ac547": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_848b78cf0cac4a82a55121a58b23551e",
              "IPY_MODEL_3577b1ad723b40458a1490cc261e7ac6",
              "IPY_MODEL_7a2c0600f7b04e04ac3b208c8f544741"
            ],
            "layout": "IPY_MODEL_9cf31f0f36b84d54bf16a9fa0e25aeb3"
          }
        },
        "6ba341d3907041ee954b82f53ce7e772": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70002f9be67549d8a9aa292835eb5f6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "721056e6e9084dabb72bfbb0be6cdef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a2c0600f7b04e04ac3b208c8f544741": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9da6d71e956e4a5b83688c6634b21070",
            "placeholder": "",
            "style": "IPY_MODEL_07685a354a794ceaab0d9bedf9c2847a",
            "value": "57/57[00:02&lt;00:00,27.41ba/s]"
          }
        },
        "7b087a4ef4f54a44a0acab4ea3f48d13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e3fd8b81572439f997f4736a4011236": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8458ae7f99e04aca8a3f629c300d7427": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "848b78cf0cac4a82a55121a58b23551e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd2a7703ed0148c2b40f8f815a2d51d5",
            "placeholder": "",
            "style": "IPY_MODEL_7e3fd8b81572439f997f4736a4011236",
            "value": "CreatingCSVfromArrowformat:100%"
          }
        },
        "8a2d7c67f450467cbcd99f137952f608": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8458ae7f99e04aca8a3f629c300d7427",
            "max": 56207,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_51b907abfbac4ac6b4bb237b488d2f0c",
            "value": 56207
          }
        },
        "91714c9a67164385b02ca5c4e586c158": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ba341d3907041ee954b82f53ce7e772",
            "placeholder": "",
            "style": "IPY_MODEL_c49539b59ced4031abc89a83fc3cdf52",
            "value": "Map:100%"
          }
        },
        "92021e24ebb049829570d00b7d295c77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be5e52b940f342cf8269becf8745043d",
            "placeholder": "",
            "style": "IPY_MODEL_9d004fca3368400eb2ee472cd76446a7",
            "value": "CreatingCSVfromArrowformat:100%"
          }
        },
        "94a23c0f6f9b48b3b2d3a4240f8ab7bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f804546c280452aa6873bc1b942dab4",
            "placeholder": "",
            "style": "IPY_MODEL_e0c050393b0444d086c3ddee21bdf2de",
            "value": "13295/13295[00:03&lt;00:00,4196.64examples/s]"
          }
        },
        "9cf31f0f36b84d54bf16a9fa0e25aeb3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d004fca3368400eb2ee472cd76446a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9da6d71e956e4a5b83688c6634b21070": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7b8aec7b3a5415b852a7179638a3e1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92021e24ebb049829570d00b7d295c77",
              "IPY_MODEL_db786769c3c949c1b88ff09f3157e918",
              "IPY_MODEL_283a3d3cd0074b92bbd41d8c0f64b6b1"
            ],
            "layout": "IPY_MODEL_114dc92a6ce845f4b888cfdedc4ed35e"
          }
        },
        "be5e52b940f342cf8269becf8745043d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c233e3f8ae68463db1ed1e5ae0105680": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70002f9be67549d8a9aa292835eb5f6f",
            "max": 13295,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_721056e6e9084dabb72bfbb0be6cdef4",
            "value": 13295
          }
        },
        "c49539b59ced4031abc89a83fc3cdf52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4a313597e5040778a0e70a3deab7a84": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca593e364b834ae99c9dec876b720c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1b192d6ece4435f928867f8835f728d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b087a4ef4f54a44a0acab4ea3f48d13",
            "placeholder": "",
            "style": "IPY_MODEL_ea0fbde2cb7540eb96bbe9caac016a15",
            "value": "56207/56207[00:19&lt;00:00,2291.52examples/s]"
          }
        },
        "d344c25b84644d21bce2b5d0126f4b10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d57632adf0d246838b7de006733a187f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db786769c3c949c1b88ff09f3157e918": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4a313597e5040778a0e70a3deab7a84",
            "max": 14,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58951bdfad414d65ac175942346b4f41",
            "value": 14
          }
        },
        "dd2a7703ed0148c2b40f8f815a2d51d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0c050393b0444d086c3ddee21bdf2de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea0fbde2cb7540eb96bbe9caac016a15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef9fd8174cb74a228d4c816f24c28081": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}